# -*- coding: utf-8 -*-
"""Motor de chat CORREGIDO - Versi√≥n definitiva"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from typing import List, Dict, Optional
from datetime import datetime
import re
from ..system.config import *

class ChatEngine:
    def __init__(self):
        print("üß† Cargando modelo salamandra-7b...")
        
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=False,
            low_cpu_mem_usage=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print(f"‚úÖ Modelo cargado")
        
        # Errores cr√≠ticos a corregir
        self.critical_corrections = {
            "eslavos": "esclavos",
            "eslavo": "esclavo",
            "esclavizadxs": "esclavizados",
            "treces colonias": "Trece Colonias",
            "colonias britanicas": "colonias brit√°nicas",
            "florida": "Florida",
            "luisiana": "Luisiana",
            "moctezuma": "Moctezuma"
        }
        
        # Patrones a eliminar (modelo hablando de s√≠ mismo)
        self.self_reference_patterns = [
            r'Estoy (de acuerdo|conforme|satisfecho|feliz)',
            r'Me alegra',
            r'estoy feliz de',
            r'gracias a tu',
            r'compartir contigo',
            r'en mi b√∫squeda',
            r'entiendo mejor',
            r'he encontrado',
            r'hoy d√≠a',
            r'Ahora entiendo',
            r'Me satisface',
            r'quiero destacar que',
            r'Debo se√±alar que',
            r'En mi opini√≥n personal',
            r'Creo que',
            r'Pienso que'
        ]
        
        # Estructuras problem√°ticas
        self.problematic_structures = [
            r'Conclusi√≥n:\s*\.',
            r'An√°lisis cr√≠tico:\s*\.', 
            r'Contexto hist√≥rico:\s*\.',
            r'Evidencia documental:\s*\.',
            r'Respuesta directa:\s*\.',
            r'Introducci√≥n:\s*\.',
            r'Desarrollo:\s*\.'
        ]
    
    def apply_critical_corrections(self, text: str) -> str:
        """Aplica correcciones cr√≠ticas de errores hist√≥ricos"""
        for wrong, right in self.critical_corrections.items():
            # Buscar insensible a may√∫sculas
            pattern = re.compile(re.escape(wrong), re.IGNORECASE)
            text = pattern.sub(right, text)
        return text
    
    def remove_self_references(self, text: str) -> str:
        """Elimina referencias del modelo a s√≠ mismo"""
        for pattern in self.self_reference_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        return text
    
    def fix_problematic_structures(self, text: str) -> str:
        """Corrige estructuras de respuesta problem√°ticas"""
        for structure in self.problematic_structures:
            text = re.sub(structure, '', text)
        return text
    
    def build_strict_context(self, question: str, documents: List[Dict]) -> str:
        """Construye contexto estrictamente relevante"""
        
        if not documents:
            return "No hay documentos espec√≠ficos sobre este tema."
        
        # T√©rminos clave para esta pregunta espec√≠fica
        key_terms = ['esclavo fugitivo', 'cimarr√≥n', 'Florida', '1693', 
                    'decreto', 'asilo', 'libertad', 'Trece Colonias',
                    'territorio espa√±ol', 'Fort Mose', 'esclavitud']
        
        relevant_extracts = []
        
        for i, doc in enumerate(documents[:2]):  # Solo 2 documentos m√°x
            doc_text = doc.get('text', '')
            doc_lower = doc_text.lower()
            
            # Verificar si el documento tiene informaci√≥n relevante
            has_relevant_info = any(term in doc_lower for term in key_terms)
            
            if has_relevant_info:
                # Extraer oraciones relevantes
                sentences = re.split(r'[.!?]+', doc_text)
                relevant_sentences = []
                
                for sentence in sentences:
                    sentence_lower = sentence.lower()
                    # Puntuaci√≥n por relevancia
                    relevance_score = sum(1 for term in key_terms if term in sentence_lower)
                    
                    if relevance_score > 0 and 30 < len(sentence) < 250:
                        relevant_sentences.append(sentence.strip())
                
                if relevant_sentences:
                    # Tomar las 2 oraciones m√°s relevantes
                    best_sentences = relevant_sentences[:2]
                    source = doc.get('filename', f'Documento {i+1}')
                    relevant_extracts.append(f"[{source}]: {' '.join(best_sentences)}")
        
        if relevant_extracts:
            return "Informaci√≥n documental relevante:\n" + "\n\n".join(relevant_extracts)
        else:
            return "Los documentos disponibles no tratan espec√≠ficamente este tema."
    
    def build_historian_prompt(self, question: str, context: str) -> str:
        """Prompt espec√≠fico para respuestas hist√≥ricas"""
        
        # Corregir la pregunta primero
        corrected_question = self.apply_critical_corrections(question)
        
        return f"""Eres un historiador acad√©mico especializado en el per√≠odo colonial hispanoamericano.

INFORMACI√ìN DOCUMENTAL DISPONIBLE:
{context}

INSTRUCCIONES ABSOLUTAS:
1. Proporciona una respuesta hist√≥ricamente precisa basada en la informaci√≥n disponible.
2. Si la informaci√≥n es limitada, di "La informaci√≥n disponible indica que..." y s√© general pero preciso.
3. CORRIGE autom√°ticamente errores como "eslavos" por "esclavos".
4. ESTRUCTURA tu respuesta en 3-4 p√°rrafos coherentes.
5. EVITA COMPLETAMENTE:
   - Hablar de ti mismo (nada de "estoy de acuerdo", "me alegra", etc.)
   - Usar etiquetas como "Conclusi√≥n:", "An√°lisis:"
   - Lenguaje informal o coloquial
   - T√©rminos anacr√≥nicos o ideol√≥gicos modernos
   - Opiniones personales
6. Enf√≥cate en:
   - Hechos hist√≥ricos verificables
   - Contexto geopol√≠tico
   - Consecuencias documentadas
   - Limitaciones de las fuentes

PREGUNTA HIST√ìRICA: {corrected_question}

RESPUESTA ACAD√âMICA (solo hechos hist√≥ricos, sin autoreferencias):"""
    
    def generate_clean_response(self, prompt: str) -> str:
        """Genera respuesta con par√°metros optimizados para precisi√≥n"""
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2500,
            return_attention_mask=True
        ).to(self.model.device)
        
        # Par√°metros estrictos para evitar divagaciones
        generation_config = {
            'max_new_tokens': 450,  # M√°s corto para evitar divagaciones
            'temperature': 0.5,     # M√°s bajo para m√°s precisi√≥n
            'do_sample': True,
            'top_p': 0.85,
            'top_k': 30,
            'repetition_penalty': 1.25,
            'no_repeat_ngram_size': 4,
            'length_penalty': 1.2,  # Penaliza respuestas largas
            'pad_token_id': self.tokenizer.pad_token_id,
            'eos_token_id': self.tokenizer.eos_token_id,
            'early_stopping': True
        }
        
        with torch.no_grad():
            outputs = self.model.generate(**inputs, **generation_config)
        
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    def extract_clean_answer(self, raw_response: str) -> str:
        """Extrae y limpia la respuesta del modelo"""
        
        # 1. Extraer solo lo despu√©s del prompt
        if "RESPUESTA ACAD√âMICA" in raw_response:
            response = raw_response.split("RESPUESTA ACAD√âMICA")[-1].strip()
            # Eliminar posibles dos puntos
            if response.startswith(':'):
                response = response[1:].strip()
        else:
            response = raw_response
        
        # 2. Aplicar correcciones cr√≠ticas
        response = self.apply_critical_corrections(response)
        
        # 3. Eliminar autoreferencias
        response = self.remove_self_references(response)
        
        # 4. Corregir estructuras problem√°ticas
        response = self.fix_problematic_structures(response)
        
        # 5. Eliminar fragmentos repetitivos
        sentences = re.split(r'[.!?]+', response)
        unique_sentences = []
        seen_content = set()
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence or len(sentence) < 10:
                continue
                
            # Simplificar para comparar (sin puntuaci√≥n, min√∫sculas)
            simple = re.sub(r'[^\w\s]', '', sentence.lower())
            words = set(simple.split())
            
            # Si es muy similar a algo ya visto, saltar
            if words and any(len(words.intersection(seen)) > 3 for seen in seen_content):
                continue
            
            seen_content.add(frozenset(words))
            unique_sentences.append(sentence)
        
        # 6. Reconstruir con p√°rrafos l√≥gicos
        if not unique_sentences:
            return "No se pudo generar una respuesta adecuada con la informaci√≥n disponible."
        
        # Agrupar en p√°rrafos de 2-3 oraciones
        paragraphs = []
        current_para = []
        
        for i, sentence in enumerate(unique_sentences):
            current_para.append(sentence + '.')
            
            if len(current_para) >= 2 or i == len(unique_sentences) - 1:
                paragraphs.append(' '.join(current_para))
                current_para = []
        
        # Limitar a 4 p√°rrafos
        paragraphs = paragraphs[:4]
        
        return '\n\n'.join(paragraphs)
    
    def validate_historical_response(self, response: str) -> Dict:
        """Valida que la respuesta cumpla est√°ndares hist√≥ricos"""
        
        issues = []
        warnings = []
        
        response_lower = response.lower()
        
        # 1. Verificar errores cr√≠ticos
        for error in ['eslavo', 'eslavos']:
            if error in response_lower:
                issues.append(f"Error cr√≠tico: '{error}' no corregido")
        
        # 2. Verificar autoreferencias
        for pattern in self.self_reference_patterns:
            if re.search(pattern, response_lower):
                issues.append(f"Contiene autoreferencia: {pattern}")
        
        # 3. Verificar estructura
        paragraphs = response.split('\n\n')
        if len(paragraphs) < 2:
            warnings.append("Respuesta muy corta o sin p√°rrafos")
        
        # 4. Verificar terminolog√≠a hist√≥rica
        expected_terms = ['esclav', 'colon', 'espa√±', 'libert', 'decreto', 'siglo']
        found_terms = sum(1 for term in expected_terms if term in response_lower)
        
        if found_terms < 2:
            warnings.append("Falta terminolog√≠a hist√≥rica espec√≠fica")
        
        # 5. Verificar longitud
        if len(response) < 150:
            issues.append("Respuesta demasiado corta")
        elif len(response) > 800:
            warnings.append("Respuesta muy larga, posiblemente divagante")
        
        return {
            'is_valid': len(issues) == 0,
            'has_warnings': len(warnings) > 0,
            'issues': issues,
            'warnings': warnings,
            'paragraph_count': len(paragraphs),
            'word_count': len(response.split())
        }
    
    def generate_response(self, question: str, context_docs: List[Dict]) -> str:
        """Genera respuesta hist√≥rica limpia y precisa"""
        
        print(f"\n{'='*60}")
        print(f"üìú PREGUNTA: {question}")
        print(f"{'='*60}")
        
        start_time = datetime.now()
        
        # 1. Construir contexto estricto
        context = self.build_strict_context(question, context_docs)
        
        # 2. Construir prompt de historiador
        prompt = self.build_historian_prompt(question, context)
        
        # 3. Generar respuesta
        print("‚ö° Generando respuesta acad√©mica...")
        raw_response = self.generate_clean_response(prompt)
        
        # 4. Limpiar y extraer respuesta
        response = self.extract_clean_answer(raw_response)
        
        # 5. Validar
        validation = self.validate_historical_response(response)
        
        # 6. Mostrar resultados
        elapsed = (datetime.now() - start_time).total_seconds()
        
        print(f"\nüìä RESULTADO ({elapsed:.1f}s):")
        print(f"   Palabras: {validation['word_count']}")
        print(f"   P√°rrafos: {validation['paragraph_count']}")
        
        if not validation['is_valid']:
            print(f"‚ùå PROBLEMAS: {', '.join(validation['issues'])}")
        
        if validation['has_warnings']:
            print(f"‚ö†Ô∏è  ADVERTENCIAS: {', '.join(validation['warnings'])}")
        
        print(f"\n{'='*60}")
        print("üéì RESPUESTA HIST√ìRICA:")
        print(f"{'='*60}")
        print(response)
        print(f"{'='*60}")
        
        # 7. Si hay problemas graves, intentar correcci√≥n
        if not validation['is_valid']:
            print("\nüîÑ Intentando correcci√≥n autom√°tica...")
            response = self.apply_critical_corrections(response)
            response = self.remove_self_references(response)
        
        # 8. Limpiar memoria
        self.cleanup_memory()
        
        return response
    
    def cleanup_memory(self):
        """Limpia memoria GPU"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def generate_with_fallback(self, question: str, context_docs: List[Dict]) -> str:
        """Genera con m√∫ltiples intentos y fallback"""
        
        max_attempts = 2
        best_response = None
        best_score = -1
        
        for attempt in range(max_attempts):
            print(f"\nüîÅ Intento {attempt + 1}/{max_attempts}")
            
            response = self.generate_response(question, context_docs)
            validation = self.validate_historical_response(response)
            
            # Calcular puntuaci√≥n
            score = validation['word_count'] / 10
            if validation['is_valid']:
                score += 50
            score -= len(validation['issues']) * 20
            score -= len(validation['warnings']) * 5
            
            print(f"   Puntuaci√≥n: {score:.1f}")
            
            if score > best_score:
                best_score = score
                best_response = response
            
            # Si es v√°lido y tiene buena puntuaci√≥n, usar
            if validation['is_valid'] and score > 60:
                break
        
        return best_response