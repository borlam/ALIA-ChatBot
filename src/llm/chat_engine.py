# -*- coding: utf-8 -*-
"""Motor de chat SIMPLIFICADO (usa an√°lisis ya hecho)"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from typing import List, Dict
import re
from datetime import datetime
from ..system.config import *

class ChatEngine:
    def __init__(self, model_key: str = None):
        """Inicializa el motor de chat con un modelo espec√≠fico"""
        
        # Usar modelo por defecto si no se especifica
        if model_key and model_key in get_available_models_list():
            set_active_model(model_key)
        
        print(f"üß† Cargando modelo {MODEL_NAME} (modo optimizado)...")
        print(f"üìä Configuraci√≥n: {MAX_TOKENS} tokens m√°x, {TEMPERATURE} temperatura")
        
        # Configurar cuantizaci√≥n seg√∫n modelo
        model_info = get_active_model_info()
        
        if "40b" in model_info["name"].lower():
            # Para ALIA-40B, usar cuantizaci√≥n m√°s agresiva
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                llm_int8_enable_fp32_cpu_offload=True  # Descargar a CPU si es necesario
            )
        else:
            # Para Salamandra 2B/7B
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
            )
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
            
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error cargando modelo {MODEL_NAME}: {e}")
            print("üîÑ Intentando cargar sin cuantizaci√≥n...")
            
            # Fallback: cargar sin cuantizaci√≥n
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                device_map="auto",
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                trust_remote_code=True
            )
        
        # Configurar tokenizer
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print(f"‚úÖ Modelo {model_info['display_name']} cargado en modo optimizado")
        
        # Almacenar info del modelo
        self.model_info = model_info

    def compute_confidence(self, documents: List[Dict]) -> str:
        # ... (mant√©n esta funci√≥n igual que antes) ...
        if not documents:
            return "low"

        scores = [doc.get("score", 0) for doc in documents[:5]]
        avg_score = sum(scores) / len(scores)

        if avg_score >= 0.75 and len(documents) >= 3:
            return "high"
        elif avg_score >= 0.55:
            return "medium"
        else:
            return "low"

    def build_intelligent_context(self, question: str, documents: List[Dict]) -> str:
        # ... (mant√©n esta funci√≥n igual que antes) ...
        if not documents:
            return ""

        parts = []

        for doc in documents[:6]:
            text = doc["text"]

            if len(text) > 400:
                text = text[:400]

            parts.append(text)

        return "\n\n".join(parts)

    def build_prompt_with_confidence(self, question: str, context: str, confidence: str) -> str:
        tone = {
            "high": "Responde con seguridad y detalle.",
            "medium": "Responde de forma natural, indicando matices si es necesario.",
            "low": (
                "Responde de forma conversacional. "
                "Si no est√°s completamente seguro, ind√≠calo de manera natural "
                "y evita afirmaciones categ√≥ricas."
            )
        }[confidence]

        context_block = ""
        if context.strip():
            context_block = f"""
Contexto documental (puede ser parcial, sesgado o no relevante para la pregunta):
{context}
"""

        return f"""
Eres regerIA, un asistente experto en historia hispanoamericana.

Tu funci√≥n es explicar procesos hist√≥ricos con rigor, claridad y sentido cr√≠tico.
No debes resumir documentos ni justificar posturas pol√≠ticas o imperiales.

Instrucciones IMPORTANTES:
- Usa el contexto SOLO si aporta informaci√≥n directamente relevante.
- Si el contexto no es pertinente, ign√≥ralo por completo.
- Distingue entre hechos hist√≥ricos comprobados y valoraciones.
- Evita idealizar o demonizar a personas, pueblos o imperios.
- No inventes datos concretos si no est√°s seguro.
- {tone}
- Responde siempre en espa√±ol, con un estilo claro y cercano.
{context_block}
Pregunta:
{question}

Respuesta:
"""


    def generate_response(self, question: str, context_docs: List[Dict], max_chars: int = 2000) -> str:
        """Genera respuesta R√ÅPIDA usando an√°lisis pre-existente"""
        
        start_time = datetime.now()
        
        confidence = self.compute_confidence(context_docs)
        context = ""
        if confidence != "low":
            context = self.build_intelligent_context(question, context_docs)
        prompt = self.build_prompt_with_confidence(question, context, confidence)

        if confidence == "high":
            temperature = 0.6
        elif confidence == "medium":
            temperature = 0.7
        else:
            temperature = 0.85

        # Ajustar tokens seg√∫n modelo
        if "40b" in self.model_info["name"].lower():
            max_length = 3500  # ALIA necesita m√°s contexto
            max_new_tokens = 800
        else:
            max_length = 2500
            max_new_tokens = MAX_TOKENS

        # 3. Tokenizaci√≥n eficiente
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=max_length
        ).to(self.model.device)
        
        # 4. Generaci√≥n con par√°metros optimizados
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=True,
                top_p=TOP_P,
                repetition_penalty=1.15,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # 5. Procesamiento simple
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extraer solo la respuesta
        if "RESPUESTA:" in response:
            response = response.split("RESPUESTA:")[-1].strip()
        elif "respuesta:" in response:
            response = response.split("respuesta:")[-1].strip()
        
        # Limitar longitud
        if len(response) > max_chars:
            if "." in response[max_chars-200:max_chars]:
                last_period = response[:max_chars].rfind(".")
                response = response[:last_period+1]
        
        # Estad√≠sticas
        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"‚úÖ Respuesta en {elapsed:.1f}s, {len(response)} caracteres (Modelo: {self.model_info['display_name']})")
        
        # Limpiar memoria
        self.cleanup_memory()
        
        return response.strip()
    
    def cleanup_memory(self):
        """Limpia memoria GPU"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def get_model_info(self):
        """Obtiene informaci√≥n del modelo actual"""
        return self.model_info