# -*- coding: utf-8 -*-
"""Motor optimizado para precisi√≥n con Salamandra-7B"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer
from typing import List, Dict
import re
from .config import *

class ChatEngine:
    def __init__(self):
        print("üß† Cargando modelo salamandra-7b (optimizado para precisi√≥n)...")
        
        # CONFIGURACI√ìN DE CUANTIZACI√ìN 4-BIT
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=False
        )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        print("üî§ Cargando embeddings para precisi√≥n...")
        self.embedder = SentenceTransformer(EMBEDDING_MODEL)
        if torch.cuda.is_available():
            self.embedder = self.embedder.to(torch.device("cuda"))
        
        print(f"‚úÖ Modelo cargado en 4-bit")
        self.print_memory_usage()

    def print_memory_usage(self):
        """Imprime uso de memoria"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            total = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"üìä Memoria GPU: {allocated:.1f}GB / {reserved:.1f}GB / {total:.1f}GB")

    def extract_key_information(self, documents: List[Dict]) -> str:
        """Extrae y resume informaci√≥n clave de m√∫ltiples documentos"""
        key_info = {}
        
        for i, doc in enumerate(documents):
            source = doc.get('pdf_title', f'Documento {i+1}')
            text = doc['text']
            
            # Extraer oraciones m√°s relevantes (primeras y con palabras clave)
            sentences = text.split('. ')
            if len(sentences) > 3:
                # Tomar primeras oraciones + algunas del medio
                selected = sentences[:2] + sentences[len(sentences)//2:len(sentences)//2+1]
                summary = '. '.join(selected) + '.'
            else:
                summary = text[:300] + '...' if len(text) > 300 else text
            
            key_info[source] = summary
        
        # Formatear informaci√≥n clave
        formatted = ""
        for source, info in key_info.items():
            formatted += f"\n\nüìÑ **{source}**:\n{info}"
        
        return formatted

    def generate_response(self, question: str, context_docs: List[Dict], max_chars: int = 2000) -> str:
        """Genera respuesta PRECISA usando TODOS los documentos relevantes"""
        
        print(f"üîç Analizando {len(context_docs)} documentos para: '{question[:80]}...'")
        
        # 1. EXTRAER INFORMACI√ìN CLAVE DE TODOS LOS DOCUMENTOS
        context_text = self.extract_key_information(context_docs)
        
        # 2. CONTAR FUENTES √öNICAS
        pdf_sources = {}
        for i, doc in enumerate(context_docs):
            source = doc.get('pdf_title', f'Documento {i+1}')
            if source not in pdf_sources:
                pdf_sources[source] = 0
            pdf_sources[source] += 1
        
        # 3. PROMPT OPTIMIZADO PARA PRECISI√ìN
        prompt = f"""### ROL:
Eres un historiador experto en cultura hisp√°nica. Tu tarea es responder PRECISAMENTE bas√°ndote EXCLUSIVAMENTE en los documentos proporcionados.

### DOCUMENTOS DE REFERENCIA ({len(context_docs)} documentos):
{context_text if context_text else "No hay documentos espec√≠ficos disponibles."}

### REGLAS ESTRICTAS:
1. Responde √öNICAMENTE con informaci√≥n presente en los documentos anteriores
2. Si algo no est√° en los documentos, di "No encuentro esa informaci√≥n en los documentos"
3. S√© preciso y cita informaci√≥n espec√≠fica cuando sea posible
4. No inventes nombres, fechas, eventos o referencias
5. Si los documentos son contradictorios, menci√≥nalo

### PREGUNTA:
{question}

### PROCESO DE AN√ÅLISIS:
1. Identificar qu√© documentos contienen informaci√≥n relevante
2. Extraer los hechos clave
3. Sintetizar una respuesta precisa

### RESPUESTA PRECISA BASADA EN DOCUMENTOS:"""

        # 4. CALCULAR TOKENS DISPONIBLES
        # Estimaci√≥n conservadora para 7B
        max_context_tokens = 2048  # L√≠mite seguro para 7B en 4-bit
        prompt_tokens = len(self.tokenizer.encode(prompt))
        available_tokens = max_context_tokens - prompt_tokens - 100  # Margen
        
        max_gen_tokens = min(600, available_tokens)  # M√°ximo razonable
        max_gen_tokens = max(100, max_gen_tokens)    # M√≠nimo razonable
        
        print(f"üìù Tokens: prompt={prompt_tokens}, disponibles={available_tokens}, generaci√≥n={max_gen_tokens}")

        # 5. TOKENIZACI√ìN CON MANEJO DE TRUNCAMIENTO
        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=max_context_tokens - 200  # Dejar espacio para respuesta
            ).to(self.model.device)
        except Exception as e:
            print(f"‚ö†Ô∏è Error tokenizando: {e}")
            # Versi√≥n de respaldo con menos contexto
            if len(context_text) > 3000:
                context_text = context_text[:3000] + "... [texto truncado por longitud]"
            prompt = prompt.replace(context_text, context_text)
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1900).to(self.model.device)

        # 6. GENERACI√ìN CON PAR√ÅMETROS PARA PRECISI√ìN
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_gen_tokens,
                min_new_tokens=80,
                temperature=0.5,  # BAJO para m√°xima precisi√≥n
                do_sample=False,   # greedy decoding para m√°s consistencia
                top_p=0.9,
                top_k=40,
                repetition_penalty=1.25,  # Alto para evitar repeticiones
                no_repeat_ngram_size=4,
                length_penalty=1.0,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True
            )

        # 7. DECODIFICACI√ìN Y VALIDACI√ìN
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extraer SOLO la parte despu√©s de "RESPUESTA PRECISA BASADA EN DOCUMENTOS:"
        response_marker = "RESPUESTA PRECISA BASADA EN DOCUMENTOS:"
        if response_marker in response:
            response = response.split(response_marker)[-1].strip()
        
        # Tambi√©n intentar con otras variantes
        for marker in ["Respuesta:", "respuesta:", "### RESPUESTA", "RESPUESTA:"]:
            if marker in response and response.find(marker) < 100:  # Solo si est√° cerca del inicio
                response = response.split(marker)[-1].strip()
        
        # 8. POST-PROCESAMIENTO PARA PRECISI√ìN
        # Eliminar cualquier referencia al prompt
        for marker in ["### ROL:", "### DOCUMENTOS:", "### REGLAS:", "### PREGUNTA:", "### PROCESO:"]:
            if marker in response:
                response = response.split(marker)[0].strip()
        
        # Verificar que no se inventen URLs
        urls = re.findall(r'https?://\S+', response)
        if urls and len(context_docs) < 3:  # Si hay URLs pero pocos documentos, sospechoso
            response = re.sub(r'https?://\S+', '[referencia a documento]', response)
        
        # 9. A√ëADIR METADATA DE FUENTES
        if pdf_sources and len(context_docs) > 0:
            sources_list = list(pdf_sources.keys())
            if sources_list:
                if len(sources_list) <= 3:
                    sources_text = ", ".join(sources_list)
                else:
                    sources_text = f"{sources_list[0]}, {sources_list[1]} y {len(sources_list)-2} m√°s"
                
                response += f"\n\n---\nüìö **Documentos consultados ({len(context_docs)}):** {sources_text}"
        
        # 10. VALIDACI√ìN DE CALIDAD
        # Verificar que no sea demasiado gen√©rica
        generic_phrases = [
            "seg√∫n los documentos", "bas√°ndome en la informaci√≥n", 
            "los documentos indican", "la informaci√≥n proporcionada"
        ]
        
        has_specific_info = any(phrase in response.lower() for phrase in generic_phrases)
        if not has_specific_info and len(context_docs) > 0:
            response += "\n\nüí° *Nota: Esta respuesta se basa en el an√°lisis de los documentos proporcionados.*"
        
        # 11. LIMITAR LONGITUD FINAL
        if len(response) > max_chars:
            # Buscar el √∫ltimo punto completo antes del l√≠mite
            if "." in response[max_chars-300:max_chars]:
                last_period = response[:max_chars].rfind(".")
                response = response[:last_period+1]
            else:
                response = response[:max_chars] + "..."
        
        print(f"‚úÖ Respuesta generada: {len(response)} caracteres, basada en {len(context_docs)} documentos")
        
        # 12. LIMPIEZA DE MEMORIA
        self.cleanup_memory()
        
        return response.strip()

    def cleanup_memory(self):
        """Limpia memoria GPU de forma segura"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            # Peque√±a pausa para permitir liberaci√≥n
            import time
            time.sleep(0.1)

    def validate_response(self, response: str, context_docs: List[Dict]) -> bool:
        """Valida que la respuesta sea coherente con los documentos"""
        if not response or len(response) < 20:
            return False
        
        # Verificar que mencione alg√∫n documento si hay documentos
        if context_docs and len(context_docs) > 0:
            doc_mentions = 0
            for doc in context_docs:
                title = doc.get('pdf_title', '').lower()
                if title and title in response.lower():
                    doc_mentions += 1
            
            # Si hay m√∫ltiples documentos pero no se menciona ninguno
            if len(context_docs) > 2 and doc_mentions == 0:
                print(f"‚ö†Ô∏è  La respuesta no menciona documentos espec√≠ficos")
                return False
        
        return True