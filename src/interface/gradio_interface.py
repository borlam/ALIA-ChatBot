# -*- coding: utf-8 -*-
"""Interfaz de usuario con Gradio adaptada a la nueva arquitectura"""

import gradio as gr
import torch
from typing import List, Tuple
from ..system.config import *

class GradioInterface:
    def __init__(self, orchestrator):
        """
        Args:
            orchestrator: Instancia de RAGOrchestrator (nueva arquitectura)
        """
        self.orchestrator = orchestrator
        print("üé® Interfaz adaptada a arquitectura optimizada")

    def create_interface(self):
        """Crea la interfaz de Gradio para la nueva arquitectura"""
        print("\n" + "="*60)
        print("üì§ CREANDO INTERFAZ GRADIO OPTIMIZADA")
        print("="*60)

        with gr.Blocks(title="RAG Hispanidad - Arquitectura Optimizada", 
                      theme=gr.themes.Soft()) as demo:
            
            # Estado
            chat_history = gr.State([])
            
            # ===== HEADER =====
            gr.Markdown("# üèõÔ∏è **RAG Hispanidad - Arquitectura Optimizada**")
            gr.Markdown("### ü§ñ Chat con PDFs Hist√≥ricos usando an√°lisis inteligente")
            gr.Markdown("""
            **Nueva arquitectura:** El an√°lisis de documentos se hace UNA VEZ durante la indexaci√≥n, 
            haciendo las consultas 10x m√°s r√°pidas y precisas.
            """)
            
            # ===== CHAT PRINCIPAL =====
            with gr.Row():
                with gr.Column(scale=2):
                    # Chatbot
                    chatbot = gr.Chatbot(
                        label="üí¨ Conversaci√≥n Inteligente",
                        height=500,
                        bubble_full_width=False,
                        avatar_images=(
                            "üïµÔ∏è",  # Usuario
                            "ü§ñ"   # Bot
                        )
                    )
                    
                    # √Årea de entrada mejorada
                    with gr.Row():
                        user_input = gr.Textbox(
                            label="Tu pregunta sobre historia hisp√°nica",
                            placeholder="Ej: ¬øQu√© documentos tienes sobre la Leyenda Negra espa√±ola?",
                            lines=3,
                            max_lines=5,
                            scale=4
                        )
                        
                        submit_btn = gr.Button(
                            "üì§ Enviar", 
                            variant="primary",
                            size="lg",
                            scale=1
                        )
                    
                    # Botones de control del chat
                    with gr.Row():
                        clear_btn = gr.Button("üóëÔ∏è Limpiar chat", variant="secondary")
                        test_btn = gr.Button("üß™ Probar sistema", variant="secondary")
                        export_btn = gr.Button("üì• Exportar conversaci√≥n", variant="secondary")
                
                # ===== PANEL LATERAL DERECHO =====
                with gr.Column(scale=1):
                    # ===== NUEVO: SELECTOR DE MODELO =====
                    gr.Markdown("### ü§ñ **Selecci√≥n de Modelo**")
                    
                    with gr.Group():
                        # Obtener modelos disponibles y modelo actual
                        try:
                            available_models = self.orchestrator.get_available_models()
                            current_model_info = self.orchestrator.get_current_model_info()
                            model_keys = list(available_models.keys())
                            current_key = current_model_info.get('key', model_keys[0] if model_keys else 'salamandra7b')
                        except:
                            # Fallback si hay error
                            available_models = {'salamandra7b': {'display_name': 'Salamandra 7B'}}
                            model_keys = ['salamandra7b']
                            current_key = 'salamandra7b'
                        
                        # Selector de modelo
                        model_selector = gr.Dropdown(
                            choices=model_keys,
                            value=current_key,
                            label="Modelo de Lenguaje",
                            info="Selecciona el modelo para generar respuestas"
                        )
                        
                        # Bot√≥n para cambiar modelo
                        change_model_btn = gr.Button(
                            "üîÑ Cambiar Modelo",
                            variant="primary",
                            size="sm"
                        )
                        
                        # Informaci√≥n del modelo actual
                        model_info_display = gr.JSON(
                            label="Informaci√≥n del Modelo",
                            value=current_model_info
                        )
                    
                    # Gesti√≥n de documentos
                    gr.Markdown("### üìÑ **Gesti√≥n de Documentos**")
                    
                    with gr.Group():
                        pdf_upload = gr.File(
                            label="Arrastra o selecciona PDFs hist√≥ricos",
                            file_types=[".pdf"],
                            file_count="multiple",
                            height=120
                        )
                        
                        with gr.Row():
                            pdf_process_btn = gr.Button(
                                "üîß Procesar & Analizar PDFs", 
                                variant="primary",
                                size="sm"
                            )
                            pdf_clear_btn = gr.Button("Limpiar", variant="secondary", size="sm")
                        
                        pdf_status = gr.Textbox(
                            label="Estado del procesamiento",
                            value="Listo para recibir PDFs...",
                            interactive=False,
                            lines=4
                        )
                    
                    # Separador
                    gr.Markdown("---")
                    
                    # Estad√≠sticas del sistema
                    gr.Markdown("### üìä **Estado del Sistema**")
                    
                    stats_display = gr.Markdown(
                        value="Cargando estad√≠sticas...",
                        label="Estad√≠sticas en tiempo real"
                    )
                    
                    # Configuraci√≥n
                    gr.Markdown("### ‚öôÔ∏è **Configuraci√≥n Avanzada**")
                    
                    with gr.Accordion("Opciones de respuesta", open=False):
                        response_length = gr.Slider(
                            minimum=500,
                            maximum=MAX_RESPONSE_LENGTH,
                            value=DEFAULT_RESPONSE_LENGTH,
                            step=100,
                            label="üìè Longitud m√°xima de respuesta"
                        )
                        
                        num_docs = gr.Slider(
                            minimum=1,
                            maximum=5,
                            value=3,
                            step=1,
                            label="üìö N√∫mero de documentos a usar"
                        )
                    
                    # Botones de sistema
                    with gr.Row():
                        refresh_btn = gr.Button("üîÑ Actualizar", variant="secondary", size="sm")
                        theme_search_btn = gr.Button("üîç Buscar por tema", variant="secondary", size="sm")
            
            # ===== INFORMACI√ìN DEL SISTEMA =====
            gr.Markdown("---")
            
            with gr.Row():
                with gr.Column(scale=1):
                    current_model = self.orchestrator.get_current_model_info()
                    model_name = current_model.get('display_name', 'Salamandra 7B')
                    
                    gr.Markdown(f"""
                    ### üèóÔ∏è **Arquitectura Optimizada**
                    - **Modelo:** {model_name}
                    - **Embeddings:** {EMBEDDING_MODEL}
                    - **Base de datos:** ChromaDB persistente
                    - **GPU:** {'‚úÖ Disponible' if torch.cuda.is_available() else '‚ùå Solo CPU'}
                    - **An√°lisis:** Durante indexaci√≥n (1x por documento)
                    """)
                
                with gr.Column(scale=2):
                    gr.Markdown("""
                    ### üéØ **C√≥mo funciona la nueva arquitectura:**
                    1. **Subes un PDF** ‚Üí Se extrae texto y se analiza COMPLETAMENTE (1 vez)
                    2. **Se indexa** ‚Üí Chunks con metadatos enriquecidos (temas, resumen)
                    3. **Haces una pregunta** ‚Üí B√∫squeda r√°pida en chunks ya analizados
                    4. **Generas respuesta** ‚Üí Usa metadatos + conocimiento general
                    
                    **Ventajas:** ‚ö° M√°s r√°pido, üß† Menos memoria, üéØ M√°s preciso
                    """)
            
            # ===== CONEXIONES =====
            # 1. CHAT PRINCIPAL
            submit_btn.click(
                fn=self.chat_function,
                inputs=[user_input, chat_history, response_length, num_docs],
                outputs=[chatbot, user_input]
            )
            
            user_input.submit(
                fn=self.chat_function,
                inputs=[user_input, chat_history, response_length, num_docs],
                outputs=[chatbot, user_input]
            )
            
            # 2. PROCESAMIENTO DE PDFs
            pdf_process_btn.click(
                fn=self.process_pdfs_function,
                inputs=[pdf_upload],
                outputs=[pdf_status, stats_display]
            )
            
            # 3. NUEVO: CAMBIO DE MODELO
            change_model_btn.click(
                fn=self.change_model_function,
                inputs=[model_selector],
                outputs=[pdf_status, model_info_display, stats_display]
            )
            
            # 4. BOTONES DE CONTROL
            clear_btn.click(
                fn=lambda: [],
                outputs=[chatbot]
            )
            
            pdf_clear_btn.click(
                fn=lambda: None,
                outputs=[pdf_upload]
            )
            
            test_btn.click(
                fn=self.test_system_function,
                inputs=[user_input, chat_history],
                outputs=[chatbot, user_input]
            )
            
            refresh_btn.click(
                fn=self.get_system_stats_markdown,
                outputs=[stats_display]
            )
            
            theme_search_btn.click(
                fn=self.search_by_theme_function,
                inputs=[user_input],
                outputs=[pdf_status]
            )
            
            # 5. Actualizar info del modelo cuando se selecciona
            model_selector.change(
                fn=lambda key: self.orchestrator.get_available_models()[key],
                inputs=[model_selector],
                outputs=[model_info_display]
            )
            
            # 6. CARGA INICIAL
            demo.load(
                fn=self.get_system_stats_markdown,
                outputs=[stats_display]
            )

        return demo

    # ===== NUEVAS FUNCIONES PARA MANEJO DE MODELOS =====
    
    def change_model_function(self, model_key: str):
        """Funci√≥n para cambiar el modelo de lenguaje"""
        print(f"\nüîÑ SOLICITUD DE CAMBIO DE MODELO: {model_key}")
        
        try:
            # Cambiar modelo usando el orquestador
            result = self.orchestrator.change_model(model_key)
            
            if result.get('success', False):
                # Obtener nueva informaci√≥n del modelo
                model_info = self.orchestrator.get_current_model_info()
                
                message = f"‚úÖ Modelo cambiado exitosamente a {model_info.get('display_name', model_key)}\n"
                message += f"üìä Ahora usar√°s: {model_info.get('description', '')}"
                
                return message, model_info, self.get_system_stats_markdown()
            else:
                error_msg = f"‚ùå Error cambiando modelo: {result.get('error', 'Error desconocido')}"
                current_info = self.orchestrator.get_current_model_info()
                return error_msg, current_info, self.get_system_stats_markdown()
                
        except Exception as e:
            print(f"‚ùå Error en change_model_function: {e}")
            current_info = self.orchestrator.get_current_model_info()
            error_msg = f"‚ùå Error cambiando modelo: {str(e)[:100]}"
            return error_msg, current_info, self.get_system_stats_markdown()
    
    # ===== FUNCIONES EXISTENTES (MODIFICADAS LEVEMENTE) =====
    
    def format_stats_detailed(self, stats):
        """Formatea las estad√≠sticas para mostrar en Markdown"""
        if not stats:
            return "üìä Estad√≠sticas no disponibles"
        
        # Obtener informaci√≥n del modelo
        model_info = self.orchestrator.get_current_model_info()
        
        md = f"""## üìä **ESTADO DEL SISTEMA**

### ü§ñ MODELO ACTIVO
‚Ä¢ **Nombre:** {model_info.get('display_name', 'Desconocido')}
‚Ä¢ **Descripci√≥n:** {model_info.get('description', 'N/A')}
‚Ä¢ **Memoria requerida:** {model_info.get('memory_required', 'N/A')}
‚Ä¢ **Compatible con GPU:** {'‚úÖ S√≠' if model_info.get('gpu_sufficient', True) else '‚ö†Ô∏è Limitada'}

### üìö DOCUMENTOS
‚Ä¢ **PDFs procesados:** {stats.get('total_pdfs', 0)}
‚Ä¢ **P√°ginas totales:** {stats.get('total_pages', 0):,}
‚Ä¢ **Chunks indexados:** {stats.get('total_chunks', 0):,}
‚Ä¢ **Calidad media:** {stats.get('quality_distribution', {}).get('alta', 0) or 'N/A'}

### ‚öôÔ∏è HARDWARE
‚Ä¢ **GPU:** {'‚úÖ ' + torch.cuda.get_device_name(0) if torch.cuda.is_available() else '‚ùå CPU'}
‚Ä¢ **Arquitectura:** {stats.get('architecture', 'optimized_v2')}
"""
        
        if torch.cuda.is_available():
            md += f"""‚Ä¢ **Memoria GPU:** {torch.cuda.memory_allocated()/1e9:.1f}GB / {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB
"""
        
        md += f"""
### üîÑ SISTEMA
‚Ä¢ **√öltima actualizaci√≥n:** {stats.get('last_update', 'N/A')}
‚Ä¢ **Embeddings:** {EMBEDDING_MODEL.split('/')[-1]}
"""
        
        return md

    def get_system_stats_markdown(self):
        """Obtiene y formatea las estad√≠sticas del sistema para Markdown"""
        stats = self.orchestrator.get_system_info()
        return self.format_stats_detailed(stats)

    def chat_function(self, message: str, history: List, max_chars: int, num_docs: int):
        """Funci√≥n principal del chat adaptada a la nueva arquitectura"""
        print(f"\n{'='*60}")
        print(f"üîî CONSULTA OPTIMIZADA: '{message[:80]}...'")
        print(f"   üìö Usando hasta {num_docs} documentos")

        try:
            # 1. Usar el NUEVO m√©todo query del orquestador
            result = self.orchestrator.query(
                question=message,
                max_docs=num_docs
            )
            
            # 2. Formatear respuesta con fuentes
            answer = result['answer']
            sources = result['sources']
            
            # 3. A√±adir informaci√≥n de fuentes a la respuesta
            if sources:
                sources_text = "\n\nüìö **Fuentes consultadas:**\n"
                for i, source in enumerate(sources):
                    has_analysis = "‚úÖ" if source.get('has_analysis') else "‚ö†Ô∏è"
                    sources_text += f"{i+1}. {has_analysis} {source['title']} (relevancia: {source['score']})\n"
                
                answer += sources_text
            
            # 4. A√±adir metadata de la respuesta
            answer += f"\n\n---\n"
            model_info = self.orchestrator.get_current_model_info()
            model_name = model_info.get('display_name', 'Desconocido')
            answer += f"üìä **Metadata:** {result['docs_used']} docs | {result['response_length']} chars | {model_name}"
            
            # 5. Actualizar historial
            history.append([message, answer])

            print(f"   ‚úÖ Respuesta generada: {result['response_length']} caracteres")
            print(f"   üìä Documentos usados: {result['docs_used']}")
            print(f"{'='*60}")

            return history, ""

        except Exception as e:
            print(f"‚ùå ERROR en chat_function: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            
            error_msg = f"""‚ö†Ô∏è **Error en el sistema optimizado**

**Detalles:** {str(e)[:150]}

üí° **Posibles soluciones:**
1. Verifica que los PDFs est√©n procesados
2. Reinicia la aplicaci√≥n si es necesario
3. Si el error persiste, revisa los logs"""
            
            history.append([message, error_msg])
            print(f"{'='*60}")
            return history, ""

    def process_pdfs_function(self, files):
        """Procesar PDFs con la nueva arquitectura"""
        if not files:
            return "‚ùå No se seleccionaron archivos", self.get_system_stats_markdown()

        print(f"\n{'='*60}")
        print(f"üì§ PROCESANDO {len(files)} PDFs CON AN√ÅLISIS COMPLETO...")
        print("(Este proceso se hace UNA VEZ por documento)")
        print(f"{'='*60}")

        results = []
        total_chunks = 0
        total_analysis_time = 0

        for i, file in enumerate(files):
            print(f"   [{i+1}/{len(files)}] Procesando '{file.name}'...")
            
            try:
                # Usar el NUEVO m√©todo process_document del orquestador
                result = self.orchestrator.process_document(file, file.name)
                
                if result.get('success', False):
                    chunks = result.get('chunks_added', 0)
                    total_chunks += chunks
                    
                    # Informaci√≥n del an√°lisis
                    themes = result.get('document_themes', [])
                    themes_text = f" | Temas: {', '.join(themes)}" if themes else ""
                    
                    results.append(f"‚úÖ {result.get('filename', 'PDF')}: {chunks} chunks{themes_text}")
                    print(f"       ‚úì {chunks} chunks, an√°lisis completado")
                else:
                    results.append(f"‚ùå {result.get('filename', 'PDF')}: {result.get('error', 'Error')}")
                    print(f"       ‚úó Error: {result.get('error', 'Error')}")
                    
            except Exception as e:
                error_msg = f"Error inesperado: {str(e)[:100]}"
                results.append(f"‚ùå {file.name}: {error_msg}")
                print(f"       ‚úó {error_msg}")

        # Actualizar estad√≠sticas
        stats = self.orchestrator.get_system_info()

        # Crear resumen detallado
        if total_chunks > 0:
            summary = f"‚úÖ **{len(files)} PDFs procesados exitosamente**\n"
            summary += f"   ‚Ä¢ **Chunks a√±adidos:** {total_chunks}\n"
            summary += f"   ‚Ä¢ **An√°lisis completado:** S√≠ (una vez por documento)\n"
            summary += f"   ‚Ä¢ **Metadatos enriquecidos:** Temas, resumen, entidades\n"
            summary += f"   ‚Ä¢ **Pr√≥ximo paso:** Ya puedes hacer preguntas sobre estos documentos"
        else:
            summary = f"‚ö†Ô∏è  {len(files)} PDFs procesados, 0 chunks a√±adidos\n"
            summary += "   Verifica que los PDFs contengan texto extra√≠ble."

        print(f"üìä RESUMEN: {summary}")
        print(f"{'='*60}")

        # Formatear resultados
        result_text = f"**Resultados del procesamiento con an√°lisis completo:**\n\n"
        result_text += summary + "\n\n"
        result_text += "**Detalles por archivo:**\n"
        result_text += "\n".join(results[:10])
        
        if len(results) > 10:
            result_text += f"\n\n... y {len(results) - 10} m√°s"

        return result_text, self.format_stats_detailed(stats)

    def test_system_function(self, message: str, history: List):
        """Funci√≥n de prueba del sistema optimizado"""
        print(f"\nüß™ PRUEBA DEL SISTEMA OPTIMIZADO")
        
        try:
            # Obtener estad√≠sticas
            stats = self.orchestrator.get_system_info()
            
            # Obtener informaci√≥n del modelo actual
            model_info = self.orchestrator.get_current_model_info()
            model_name = model_info.get('display_name', 'Salamandra-7B')
            
            # Informaci√≥n de GPU
            gpu_info = ""
            if torch.cuda.is_available():
                gpu_info = f"""
‚Ä¢ **GPU:** {torch.cuda.get_device_name(0)}
‚Ä¢ **Memoria:** {torch.cuda.memory_allocated()/1e9:.1f}GB / {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB
"""
            else:
                gpu_info = "‚Ä¢ **GPU:** ‚ùå No disponible (usando CPU)"
            
            test_response = f"""üß™ **Prueba del sistema optimizado completada**

‚úÖ **COMPONENTES VERIFICADOS:**
‚Ä¢ **Arquitectura:** Optimizada v2.0 (an√°lisis en indexaci√≥n)
‚Ä¢ **Modelo LLM:** {model_name}
‚Ä¢ **Base de vectores:** {stats.get('total_chunks', 0):,} chunks con metadatos enriquecidos
‚Ä¢ **Documentos procesados:** {stats.get('total_pdfs', 0)}
{gpu_info.strip()}

üìä **ESTAD√çSTICAS ACTUALES:**
‚Ä¢ **PDFs procesados:** {stats.get('total_pdfs', 0)}
‚Ä¢ **Chunks indexados:** {stats.get('total_chunks', 0):,}
‚Ä¢ **√öltima actualizaci√≥n:** {stats.get('last_update', 'N/A')}
‚Ä¢ **Arquitectura:** {stats.get('architecture', 'optimized_v2')}

‚ö° **VENTAJAS ACTIVAS:**
1. ‚ö° An√°lisis durante indexaci√≥n (10x m√°s r√°pido)
2. üß† Metadatos enriquecidos en cada chunk
3. üéØ B√∫squeda inteligente por temas
4. üìà Escalable a cientos de documentos

üí° **Sistema listo para uso √≥ptimo.**"""
            
            history.append([message or "Prueba del sistema", test_response])
            return history, ""
            
        except Exception as e:
            error_msg = f"‚ùå Error en prueba del sistema: {str(e)[:100]}"
            history.append([message or "Prueba", error_msg])
            return history, ""

    def search_by_theme_function(self, theme_query: str):
        """Busca documentos por tema usando an√°lisis previo"""
        if not theme_query or len(theme_query.strip()) < 3:
            return "‚ùå Por favor, ingresa un tema de b√∫squeda (m√≠nimo 3 caracteres)"
        
        print(f"\nüîç BUSCANDO POR TEMA: '{theme_query}'")
        
        try:
            # Usar el nuevo m√©todo del orquestador
            results = self.orchestrator.search_by_theme(theme_query)
            
            if not results:
                return f"üîç No encontr√© documentos con el tema: '{theme_query}'"
            
            # Formatear resultados
            response = f"**üìö Documentos encontrados para el tema: '{theme_query}'**\n\n"
            
            for i, result in enumerate(results[:5]):  # M√°ximo 5 resultados
                themes = result.get('themes', [])
                themes_text = ', '.join(themes[:3]) if themes else 'Sin temas identificados'
                
                response += f"{i+1}. **{result.get('title', 'Documento sin t√≠tulo')}**\n"
                response += f"   ‚Ä¢ Temas: {themes_text}\n"
                response += f"   ‚Ä¢ Resumen: {result.get('summary', '')[:150]}...\n\n"
            
            if len(results) > 5:
                response += f"\n... y {len(results) - 5} documentos m√°s."
            
            print(f"   ‚úÖ Encontrados {len(results)} documentos")
            return response
            
        except Exception as e:
            print(f"‚ùå Error en b√∫squeda por tema: {e}")
            return f"‚ùå Error al buscar por tema: {str(e)[:100]}"