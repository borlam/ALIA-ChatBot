{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RGcsQMbpclqu",
        "outputId": "bccc7c5c-7f31-491b-923b-3b902359e32c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r            \rGet:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://cli.github.com/packages stable/main amd64 Packages [345 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,233 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,592 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,637 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,600 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,969 kB]\n",
            "Get:22 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,864 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Fetched 38.4 MB in 5s (7,957 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils tesseract-ocr-spa\n",
            "0 upgraded, 2 newly installed, 0 to remove and 81 not upgraded.\n",
            "Need to get 1,137 kB of archives.\n",
            "After this operation, 3,006 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.12 [186 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-spa all 1:4.00~git30-7274cfa-1.1 [951 kB]\n",
            "Fetched 1,137 kB in 2s (515 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.12_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Selecting previously unselected package tesseract-ocr-spa.\n",
            "Preparing to unpack .../tesseract-ocr-spa_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-spa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-spa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytesseract, pdf2image\n",
            "Successfully installed pdf2image-1.17.0 pytesseract-0.3.13\n",
            "tesseract 4.1.1\n",
            " leptonica-1.82.0\n",
            "  libgif 5.1.9 : libjpeg 8d (libjpeg-turbo 2.1.1) : libpng 1.6.37 : libtiff 4.3.0 : zlib 1.2.11 : libwebp 1.2.2 : libopenjp2 2.4.0\n",
            " Found AVX512BW\n",
            " Found AVX512F\n",
            " Found AVX2\n",
            " Found AVX\n",
            " Found FMA\n",
            " Found SSE\n",
            " Found libarchive 3.6.0 zlib/1.2.11 liblzma/5.2.5 bz2lib/1.0.8 liblz4/1.9.3 libzstd/1.4.8\n"
          ]
        }
      ],
      "source": [
        "# Instala Tesseract OCR y Poppler\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-spa poppler-utils\n",
        "\n",
        "# Instala las bibliotecas Python necesarias\n",
        "!pip install pytesseract pdf2image pillow\n",
        "\n",
        "# Verifica la instalación\n",
        "!tesseract --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "atikwYSBDhOk",
        "outputId": "ff6ed626-70f4-4271-8287-a2d41bee85da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m311.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m249.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m322.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m384.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m289.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=51488806 sha256=236848e7d07f82a638268795e314b59c4022f6b82bd05550157990a0c80f5ca8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5pqdj149/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 diskcache-5.6.3 jinja2-3.1.6 llama-cpp-python-0.3.16 numpy-2.4.1 typing-extensions-4.15.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: typer 0.20.0 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.1/305.1 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.55.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 11.0.3 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "dataproc-spark-connect 1.0.1 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --no-cache-dir\n",
        "# Instalar transformers y dependencias principales\n",
        "!pip install transformers sentence-transformers -q\n",
        "!pip install chromadb==0.4.22 pypdf PyPDF2 pdfplumber pymupdf Pillow -q\n",
        "!pip install gradio==4.12.0 accelerate bitsandbytes -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h3cuEqn4BJsx",
        "outputId": "cbbe3aec-dd66-43fb-b173-3a6e406c33ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ALIA-ChatBot'...\n",
            "remote: Enumerating objects: 370, done.\u001b[K\n",
            "remote: Counting objects: 100% (167/167), done.\u001b[K\n",
            "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
            "remote: Total 370 (delta 91), reused 106 (delta 44), pack-reused 203 (from 1)\u001b[K\n",
            "Receiving objects: 100% (370/370), 203.14 KiB | 20.31 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n",
            "/content/ALIA-ChatBot\n",
            "Branch 'gguf' set up to track remote branch 'gguf' from 'origin'.\n",
            "Switched to a new branch 'gguf'\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.3.0 (from -r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.3.0%2Bcu118-cp312-cp312-linux_x86_64.whl (839.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m839.6/839.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.18.0 (from -r requirements.txt (line 6))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.18.0%2Bcu118-cp312-cp312-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.3.0 (from -r requirements.txt (line 7))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.0%2Bcu118-cp312-cp312-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (4.57.3)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (5.2.0)\n",
            "Requirement already satisfied: accelerate>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (1.12.0)\n",
            "Requirement already satisfied: gradio==4.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (4.12.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 21)) (0.3.13)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 22)) (1.26.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 24)) (4.67.1)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (1.2.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (0.36.0)\n",
            "Requirement already satisfied: scipy>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (1.16.3)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->-r requirements.txt (line 5)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->-r requirements.txt (line 5)) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->-r requirements.txt (line 5)) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->-r requirements.txt (line 5)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->-r requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->-r requirements.txt (line 5)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m145.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.7.0.84 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.20.5 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.20.5-py3-none-manylinux2014_x86_64.whl (142.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.9/142.9 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.3.0->-r requirements.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.0->-r requirements.txt (line 6)) (10.4.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==0.8.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (0.8.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (6.5.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (3.10.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (6.0.3)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.9 in /usr/local/lib/python3.12/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.12.0->-r requirements.txt (line 18)) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.12.0->-r requirements.txt (line 18)) (0.38.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==0.8.0->gradio==4.12.0->-r requirements.txt (line 18)) (11.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 11)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 11)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 11)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.2.2->-r requirements.txt (line 12)) (1.6.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.24.0->-r requirements.txt (line 13)) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 23)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 23)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 23)) (2025.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->-r requirements.txt (line 26)) (1.2.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.12.0->-r requirements.txt (line 18)) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.12.0->-r requirements.txt (line 18)) (2.13.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.12.0->-r requirements.txt (line 18)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.12.0->-r requirements.txt (line 18)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.12.0->-r requirements.txt (line 18)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.12.0->-r requirements.txt (line 18)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.12.0->-r requirements.txt (line 18)) (3.2.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.12.0->-r requirements.txt (line 18)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.12.0->-r requirements.txt (line 18)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.12.0->-r requirements.txt (line 18)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 23)) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.12.0->-r requirements.txt (line 18)) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.12.0->-r requirements.txt (line 18)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.12.0->-r requirements.txt (line 18)) (13.9.4)\n",
            "\u001b[33mWARNING: typer 0.20.0 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.14.0->gradio==4.12.0->-r requirements.txt (line 18)) (0.16.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==4.12.0->-r requirements.txt (line 18)) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==4.12.0->-r requirements.txt (line 18)) (0.0.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->gradio==4.12.0->-r requirements.txt (line 18)) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->gradio==4.12.0->-r requirements.txt (line 18)) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->gradio==4.12.0->-r requirements.txt (line 18)) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->gradio==4.12.0->-r requirements.txt (line 18)) (3.11)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.36.0->-r requirements.txt (line 11)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.36.0->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->-r requirements.txt (line 12)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->-r requirements.txt (line 12)) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.12.0->-r requirements.txt (line 18)) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.12.0->-r requirements.txt (line 18)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.12.0->-r requirements.txt (line 18)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.12.0->-r requirements.txt (line 18)) (0.30.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.12.0->-r requirements.txt (line 18)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.12.0->-r requirements.txt (line 18)) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.12.0->-r requirements.txt (line 18)) (0.1.2)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cu126\n",
            "    Uninstalling torchvision-0.24.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.24.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.9.0+cu126\n",
            "    Uninstalling torchaudio-2.9.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.20.5 nvidia-nvtx-cu11-11.8.86 torch-2.3.0+cu118 torchaudio-2.3.0+cu118 torchvision-0.18.0+cu118\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/borlam/ALIA-ChatBot.git\n",
        "%cd ALIA-ChatBot\n",
        "!git checkout gguf\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVR6ed8rAmzg",
        "outputId": "e5cf0b1b-975c-4d4b-8c18-5109e85912e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81o0N0piBcLv",
        "outputId": "8a229e83-fc2b-479e-d0d0-0f8caa603717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Token configurado\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "print(\"✅ Token configurado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtjqMzI35gS5",
        "outputId": "e1535fd3-f8b6-4ae2-92d0-42eb5ef3f511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ALIA-ChatBot\n",
            "\n",
            "================================================================================\n",
            "🏛️  SISTEMA RAG HISPANIDAD - ARQUITECTURA OPTIMIZADA v3.0\n",
            "🤖 CON SELECCIÓN DE MODELOS: Salamandra-2B/7B o ALIA-40B\n",
            "================================================================================\n",
            "\n",
            "🤖 MODELOS DISPONIBLES:\n",
            "--------------------------------------------------\n",
            "   🔘 salamandra2b:\n",
            "      📝 Nombre: Salamandra 2B\n",
            "      📋 Descripción: Modelo ligero y rápido\n",
            "      💾 Memoria: 2-3 GB\n",
            "      🎯 Tokens máx: 512\n",
            "      🖥️  GPU: ✅ Suficiente\n",
            "\n",
            "   🔘 salamandra7b:\n",
            "      📝 Nombre: Salamandra 7B\n",
            "      📋 Descripción: Modelo equilibrado (por defecto)\n",
            "      💾 Memoria: 4-6 GB\n",
            "      🎯 Tokens máx: 600\n",
            "      🖥️  GPU: ✅ Suficiente\n",
            "\n",
            "   🔘 alia40b:\n",
            "      📝 Nombre: ALIA 40B GGUF\n",
            "      📋 Descripción: Modelo avanzado GGUF cuantizado\n",
            "      💾 Memoria: 15-18 GB\n",
            "      🎯 Tokens máx: 800\n",
            "      🖥️  GPU: ✅ Suficiente\n",
            "\n",
            "💡 RECOMENDACIÓN (GPU: 42.5GB):\n",
            "   → Puedes usar ALIA-40B para máxima calidad\n",
            "\n",
            "1️⃣ CONFIGURANDO ENTORNO\n",
            "🔧 Configurando entorno...\n",
            "   🖥️  Google Colab detectado\n",
            "   📁 Usando ruta de Drive: /content/drive/MyDrive/RAG_Hispanidad\n",
            "   📂 /content/drive/MyDrive/RAG_Hispanidad\n",
            "   📂 /content/drive/MyDrive/RAG_Hispanidad/vector_db\n",
            "   📂 /content/drive/MyDrive/RAG_Hispanidad/pdf_storage\n",
            "   📂 /content/drive/MyDrive/RAG_Hispanidad/cache\n",
            "   ✅ Directorios creados en Google Drive\n",
            "\n",
            "2️⃣ VERIFICANDO ESTRUCTURA\n",
            "\n",
            "🔍 Verificando estructura de archivos...\n",
            "📁 Directorios requeridos:\n",
            "   ✅ src/core\n",
            "   ✅ src/processing\n",
            "   ✅ src/vector\n",
            "   ✅ src/llm\n",
            "   ✅ src/interface\n",
            "   ✅ src/system\n",
            "\n",
            "📄 Archivos requeridos:\n",
            "   ✅ src/core/document_analyzer.py\n",
            "   ✅ src/processing/pdf_manager.py\n",
            "   ✅ src/vector/vector_store.py\n",
            "   ✅ src/llm/chat_engine.py\n",
            "   ✅ src/interface/gradio_interface.py\n",
            "   ✅ src/system/rag_orchestrator.py\n",
            "   ✅ src/system/config.py\n",
            "\n",
            "📊 Total archivos Python en src: 15\n",
            "\n",
            "3️⃣ PROBANDO IMPORTS CRÍTICOS\n",
            "\n",
            "🧪 Probando imports críticos...\n",
            "   ✅ src.system.rag_orchestrator.RAGOrchestrator\n",
            "   ✅ src.core.document_analyzer.DocumentAnalyzer\n",
            "   ✅ src.llm.chat_engine.ChatEngine\n",
            "   ✅ src.vector.vector_store.PersistentVectorStore\n",
            "   ✅ src.interface.gradio_interface.GradioInterface\n",
            "\n",
            "💻 INFORMACIÓN DEL SISTEMA:\n",
            "   Python: 3.12.12\n",
            "   PyTorch: 2.3.0+cu118\n",
            "   CUDA disponible: ✅ Sí\n",
            "   GPU: NVIDIA A100-SXM4-40GB\n",
            "   Memoria GPU: 42.5 GB\n",
            "   Gradio: 4.12.0\n",
            "\n",
            "4️⃣ CARGANDO MÓDULOS DEL SISTEMA...\n",
            "✅ Módulos cargados exitosamente\n",
            "   🏗️  Arquitectura: Optimizada (análisis en indexación)\n",
            "   📁 Datos: /content/drive/MyDrive/RAG_Hispanidad\n",
            "\n",
            "============================================================\n",
            "🚀 INICIALIZANDO SISTEMA RAG OPTIMIZADO\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "🏛️  INICIALIZANDO SISTEMA RAG (ARQUITECTURA OPTIMIZADA)\n",
            "============================================================\n",
            "📄 Extractor de PDFs inicializado (OCR: ✅)\n",
            "   Idioma OCR: spa\n",
            "📋 Log cargado: 6 PDFs procesados\n",
            "📁 PDF Manager inicializado en: /content/drive/MyDrive/RAG_Hispanidad/pdf_storage\n",
            "   OCR: Habilitado\n",
            "   Idioma OCR: spa\n",
            "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "📚 Colección cargada: hispanidad_docs (967 chunks)\n",
            "\n",
            "🧠 INICIALIZANDO CHAT ENGINE\n",
            "   Modelo solicitado: salamandra7b\n",
            "   Cambiando modelo activo a: salamandra7b\n",
            "✅ Modelo cambiado a: Salamandra 7B\n",
            "   🏷️  Tipo: transformers\n",
            "   💾 Memoria requerida: 4-6 GB\n",
            "✅ Modelo cambiado a: Salamandra 7B\n",
            "   📝 Descripción: Modelo equilibrado (por defecto)\n",
            "   💾 Memoria requerida: 4-6 GB\n",
            "   Configuración cargada: BSC-LT/salamandra-7b-instruct\n",
            "\n",
            "🧠 Cargando modelo BSC-LT/salamandra-7b-instruct (modo optimizado)...\n",
            "📊 Configuración: 600 tokens máx, 0.5 temperatura\n",
            "tokenizer_config.json: 3.80kB [00:00, 2.45MB/s]\n",
            "tokenizer.model: 100% 4.81M/4.81M [00:01<00:00, 3.55MB/s]\n",
            "tokenizer.json: 100% 19.1M/19.1M [00:00<00:00, 33.7MB/s]\n",
            "special_tokens_map.json: 100% 513/513 [00:00<00:00, 5.00MB/s]\n",
            "config.json: 100% 730/730 [00:00<00:00, 8.21MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2026-01-11 06:16:19.898045: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-11 06:16:19.917172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768112179.936221   11405 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768112179.941833   11405 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768112179.956821   11405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768112179.956864   11405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768112179.956867   11405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768112179.956870   11405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-11 06:16:19.961476: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors.index.json: 23.9kB [00:00, 93.5MB/s]\n",
            "Fetching 4 files:   0% 0/4 [00:00<?, ?it/s]\n",
            "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   0% 0.00/3.46G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:   0% 0.00/2.10G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:   0% 750k/4.98G [00:01<3:16:20, 423kB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 67.8M/4.98G [00:01<01:44, 46.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:   0% 807k/2.10G [00:01<1:26:22, 405kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 202M/4.98G [00:02<00:29, 161MB/s]  \u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 537M/4.98G [00:02<00:08, 514MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 738M/4.98G [00:02<00:06, 643MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   0% 774k/3.46G [00:02<3:01:59, 317kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 940M/4.98G [00:02<00:04, 819MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:   1% 67.1M/5.00G [00:02<03:37, 22.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   2% 67.9M/3.46G [00:03<01:55, 29.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:   3% 67.9M/2.10G [00:03<01:14, 27.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   4% 135M/3.46G [00:03<00:52, 63.6MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:   6% 135M/2.10G [00:03<00:33, 57.7MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   6% 202M/3.46G [00:03<00:34, 94.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  10% 202M/2.10G [00:03<00:21, 89.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.14G/4.98G [00:03<00:10, 382MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  10% 336M/3.46G [00:04<00:22, 138MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  16% 336M/2.10G [00:04<00:13, 134MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.28G/4.98G [00:04<00:14, 250MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:   3% 135M/5.00G [00:04<02:46, 29.1MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  19% 403M/2.10G [00:04<00:14, 115MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:   5% 269M/5.00G [00:05<01:19, 59.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  12% 403M/3.46G [00:05<00:36, 84.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  22% 470M/2.10G [00:05<00:15, 107MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:   7% 336M/5.00G [00:06<01:07, 69.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  16% 537M/3.46G [00:06<00:30, 96.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  26% 537M/2.10G [00:06<00:18, 83.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:   8% 403M/5.00G [00:07<01:14, 61.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  17% 604M/3.46G [00:07<00:33, 85.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  32% 671M/2.10G [00:07<00:14, 102MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.41G/4.98G [00:07<00:32, 109MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  19% 671M/3.46G [00:08<00:31, 88.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  35% 738M/2.10G [00:08<00:13, 100MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:   9% 470M/5.00G [00:10<01:56, 38.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  38% 805M/2.10G [00:10<00:20, 62.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  23% 805M/3.46G [00:10<00:36, 72.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  42% 872M/2.10G [00:10<00:14, 82.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  25% 872M/3.46G [00:10<00:28, 91.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  45% 940M/2.10G [00:11<00:10, 107MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  27% 940M/3.46G [00:11<00:21, 115MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  12% 604M/5.00G [00:11<01:11, 61.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.48G/4.98G [00:11<00:59, 59.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  51% 1.07G/2.10G [00:12<00:10, 102MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  31% 1.07G/3.46G [00:12<00:23, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  13% 671M/5.00G [00:12<01:10, 61.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.61G/4.98G [00:12<00:46, 72.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  54% 1.14G/2.10G [00:14<00:15, 60.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  33% 1.14G/3.46G [00:14<00:36, 64.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.68G/4.98G [00:15<00:56, 58.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  15% 738M/5.00G [00:15<01:30, 47.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.75G/4.98G [00:15<00:45, 71.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  61% 1.27G/2.10G [00:15<00:08, 95.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  65% 1.36G/2.10G [00:15<00:05, 123MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  35% 1.21G/3.46G [00:15<00:32, 68.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.88G/4.98G [00:16<00:35, 88.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  16% 806M/5.00G [00:16<01:22, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  68% 1.43G/2.10G [00:16<00:06, 109MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  41% 1.41G/3.46G [00:16<00:16, 121MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.95G/4.98G [00:16<00:34, 89.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  17% 872M/5.00G [00:16<01:11, 57.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  43% 1.48G/3.46G [00:16<00:16, 117MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  71% 1.49G/2.10G [00:19<00:10, 57.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  45% 1.54G/3.46G [00:19<00:26, 71.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 2.01G/4.98G [00:22<01:27, 33.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  74% 1.56G/2.10G [00:22<00:14, 36.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  47% 1.61G/3.46G [00:22<00:44, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  19% 940M/5.00G [00:22<02:34, 26.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  21% 1.07G/5.00G [00:22<01:23, 46.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.08G/4.98G [00:23<01:06, 43.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  50% 1.75G/3.46G [00:23<00:25, 67.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  84% 1.76G/2.10G [00:23<00:04, 74.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.15G/4.98G [00:23<00:50, 56.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  23% 1.14G/5.00G [00:23<01:05, 58.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  90% 1.90G/2.10G [00:23<00:01, 102MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  56% 1.95G/3.46G [00:23<00:13, 115MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.21G/4.98G [00:23<00:38, 71.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  58% 2.01G/3.46G [00:23<00:10, 132MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  24% 1.21G/5.00G [00:23<00:53, 71.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  60% 2.08G/3.46G [00:23<00:09, 144MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.35G/4.98G [00:23<00:24, 108MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  26% 1.28G/5.00G [00:24<00:48, 76.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  94% 1.96G/2.10G [00:24<00:01, 86.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.42G/4.98G [00:24<00:25, 102MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  27% 1.34G/5.00G [00:24<00:41, 87.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  62% 2.15G/3.46G [00:24<00:11, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.48G/4.98G [00:25<00:24, 103MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  64% 2.21G/3.46G [00:25<00:12, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.62G/4.98G [00:25<00:17, 136MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors:  97% 2.03G/2.10G [00:25<00:00, 74.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  28% 1.41G/5.00G [00:26<00:53, 67.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  66% 2.28G/3.46G [00:26<00:11, 98.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.68G/4.98G [00:26<00:18, 122MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  30% 1.48G/5.00G [00:27<00:48, 73.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  68% 2.35G/3.46G [00:27<00:11, 97.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.75G/4.98G [00:29<00:36, 61.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  31% 1.54G/5.00G [00:29<01:07, 51.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  70% 2.42G/3.46G [00:29<00:17, 60.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors: 100% 2.10G/2.10G [00:29<00:00, 71.5MB/s]\n",
            "\n",
            "model-00001-of-00004.safetensors:  57% 2.82G/4.98G [00:29<00:27, 79.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  34% 1.68G/5.00G [00:29<00:37, 87.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.89G/4.98G [00:29<00:19, 105MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  72% 2.48G/3.46G [00:29<00:12, 77.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  74% 2.55G/3.46G [00:29<00:09, 98.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  36% 1.81G/5.00G [00:30<00:27, 117MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  75% 2.59G/3.46G [00:30<00:08, 103MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.02G/4.98G [00:30<00:14, 140MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.09G/4.98G [00:30<00:14, 133MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  77% 2.66G/3.46G [00:30<00:08, 100MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  38% 1.88G/5.00G [00:30<00:28, 110MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.16G/4.98G [00:31<00:14, 122MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  39% 1.95G/5.00G [00:31<00:27, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  79% 2.72G/3.46G [00:33<00:13, 53.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.22G/4.98G [00:33<00:24, 70.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  40% 2.01G/5.00G [00:33<00:42, 69.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  42% 2.08G/5.00G [00:33<00:32, 90.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  43% 2.15G/5.00G [00:33<00:24, 115MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  83% 2.86G/3.46G [00:33<00:06, 89.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  46% 2.28G/5.00G [00:33<00:14, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  84% 2.92G/3.46G [00:33<00:04, 111MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  47% 2.35G/5.00G [00:34<00:13, 197MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.36G/4.98G [00:34<00:17, 91.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  48% 2.42G/5.00G [00:34<00:14, 178MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.43G/4.98G [00:34<00:15, 102MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  50% 2.48G/5.00G [00:35<00:16, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  86% 2.99G/3.46G [00:35<00:05, 79.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  88% 3.06G/3.46G [00:37<00:07, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.49G/4.98G [00:37<00:26, 56.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  51% 2.55G/5.00G [00:37<00:34, 70.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  52% 2.62G/5.00G [00:37<00:25, 93.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  92% 3.19G/3.46G [00:37<00:02, 96.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.63G/4.98G [00:37<00:14, 93.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  94% 3.26G/3.46G [00:37<00:01, 117MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  54% 2.68G/5.00G [00:37<00:20, 115MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.69G/4.98G [00:38<00:12, 102MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  96% 3.33G/3.46G [00:38<00:01, 128MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.76G/4.98G [00:38<00:10, 121MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  55% 2.75G/5.00G [00:38<00:19, 115MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.83G/4.98G [00:38<00:08, 129MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  56% 2.82G/5.00G [00:38<00:17, 123MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.89G/4.98G [00:39<00:09, 121MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  98% 3.39G/3.46G [00:39<00:00, 91.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  58% 2.89G/5.00G [00:41<00:36, 57.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.96G/4.98G [00:41<00:15, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.09G/4.98G [00:41<00:07, 113MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  60% 3.02G/5.00G [00:41<00:20, 96.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  85% 4.23G/4.98G [00:41<00:04, 166MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  64% 3.22G/5.00G [00:42<00:10, 167MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.30G/4.98G [00:42<00:03, 180MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors: 100% 3.46G/3.46G [00:42<00:00, 81.3MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  66% 3.29G/5.00G [00:42<00:11, 153MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.38G/4.98G [00:42<00:03, 161MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  67% 3.36G/5.00G [00:43<00:10, 151MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.45G/4.98G [00:43<00:03, 157MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.52G/4.98G [00:51<00:16, 27.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  69% 3.42G/5.00G [00:51<00:53, 29.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  79% 3.96G/5.00G [00:51<00:09, 106MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.79G/4.98G [00:51<00:02, 66.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  83% 4.16G/5.00G [00:51<00:05, 142MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.92G/4.98G [00:51<00:00, 91.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [00:52<00:00, 95.7MB/s]\n",
            "Fetching 4 files:  25% 1/4 [00:52<02:37, 52.56s/it]\n",
            "\n",
            "model-00002-of-00004.safetensors:  91% 4.56G/5.00G [00:52<00:01, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors:  95% 4.76G/5.00G [00:52<00:00, 345MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [00:52<00:00, 95.4MB/s]\n",
            "Fetching 4 files: 100% 4/4 [00:52<00:00, 13.21s/it]\n",
            "Loading checkpoint shards: 100% 4/4 [00:14<00:00,  3.72s/it]\n",
            "generation_config.json: 100% 200/200 [00:00<00:00, 1.68MB/s]\n",
            "✅ Modelo Salamandra 7B cargado en modo optimizado\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n",
            "\n",
            "✅ SISTEMA OPTIMIZADO LISTO\n",
            "🤖 Modelo activo: Salamandra 7B\n",
            "📊 Documentos: 0\n",
            "📊 Chunks: 967\n",
            "⚡ Arquitectura: Análisis en indexación\n",
            "💾 GPU: ✅ NVIDIA A100-SXM4-40GB\n",
            "\n",
            "📊 SISTEMA CARGADO EXITOSAMENTE:\n",
            "   • PDFs procesados: 0\n",
            "   • Chunks indexados: 967\n",
            "   • GPU activa: ✅ Sí\n",
            "   • Modelo activo: Salamandra 7B\n",
            "   • Descripción: Modelo equilibrado (por defecto)\n",
            "   • Arquitectura: optimized_v2\n",
            "\n",
            "5️⃣ CREANDO INTERFAZ WEB...\n",
            "🎨 Interfaz adaptada a arquitectura optimizada\n",
            "🤖 Selector de modelos habilitado\n",
            "\n",
            "============================================================\n",
            "📤 CREANDO INTERFAZ GRADIO OPTIMIZADA CON SELECTOR DE MODELOS\n",
            "============================================================\n",
            "\n",
            "🔍 VERIFICANDO MÉTODOS DEL ORCHESTRATOR:\n",
            "   get_available_models: True\n",
            "   get_current_model_info: True\n",
            "   change_model: True\n",
            "\n",
            "📊 MODELOS DISPONIBLES: ['salamandra2b', 'salamandra7b', 'alia40b']\n",
            "📊 MODELO ACTUAL: {'name': 'BSC-LT/salamandra-7b-instruct', 'display_name': 'Salamandra 7B', 'description': 'Modelo equilibrado (por defecto)', 'memory_required': '4-6 GB', 'max_tokens': 600, 'key': 'salamandra7b', 'gpu_sufficient': True}\n",
            "\n",
            "============================================================\n",
            "🌐 LANZANDO INTERFAZ WEB\n",
            "============================================================\n",
            "\n",
            "🎯 **INSTRUCCIONES DE USO:**\n",
            "1. 🤖 Selecciona el modelo en el panel derecho (2B, 7B o ALIA-40B)\n",
            "2. 📤 Sube PDFs históricos usando el panel izquierdo\n",
            "3. 🔧 Haz clic en 'Procesar PDFs' para indexarlos (con análisis completo)\n",
            "4. 💬 Pregunta sobre cualquier tema histórico\n",
            "5. 📚 Las respuestas usarán análisis previo + conocimiento general\n",
            "6. 💾 Todo se guarda automáticamente en Google Drive\n",
            "\n",
            "⚡ **VENTAJAS DE LA NUEVA ARQUITECTURA:**\n",
            "   • ⚡ 10x más rápido: Análisis se hace una sola vez\n",
            "   • 🧠 Menos memoria: Sin análisis pesado en cada pregunta\n",
            "   • 🎯 Más preciso: Metadatos enriquecidos\n",
            "   • 🤖 Modelos múltiples: Elige entre 2B, 7B o ALIA-40B\n",
            "   • 📈 Escalable: Soporta cientos de PDFs\n",
            "\n",
            "⏳ Generando URL pública...\n",
            "   La URL estará disponible en unos segundos\n",
            "   ⚠️  En Colab free, la sesión expira después de un tiempo\n",
            "   💡 Usa Ctrl+C para detener y liberar recursos\n",
            "\n",
            "🔄 **CAMBIO DE MODELO DURANTE LA EJECUCIÓN:**\n",
            "   • Selecciona un modelo diferente en el panel derecho\n",
            "   • Haz clic en '🔄 Cambiar Modelo'\n",
            "   • El sistema recargará automáticamente el nuevo modelo\n",
            "   • ⚠️ El cambio puede tardar 1-2 minutos dependiendo del modelo\n",
            "\n",
            "============================================================\n",
            "✅ SISTEMA LISTO - ESPERANDO CONEXIONES...\n",
            "============================================================\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "IMPORTANT: You are using gradio version 4.12.0, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://71c9691a5bdd26cfb9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "🔍 Modelo seleccionado: alia40b\n",
            "🧹 INICIANDO LIBERACIÓN COMPLETA DE VRAM...\n",
            "🔥 Limpieza agresiva CUDA...\n",
            "✅ VRAM liberada: 0.0GB / 0.0GB\n",
            "✅ Modelo completamente descargado\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   🏷️  Tipo: gguf\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   📝 Descripción: Modelo avanzado GGUF cuantizado\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "🔄 Cambiando de modelo: liberando anterior...\n",
            "🧹 INICIANDO LIBERACIÓN COMPLETA DE VRAM...\n",
            "🔥 Limpieza agresiva CUDA...\n",
            "✅ VRAM liberada: 0.0GB / 0.0GB\n",
            "✅ Modelo completamente descargado\n",
            "\n",
            "🧠 INICIALIZANDO CHAT ENGINE\n",
            "   Modelo solicitado: alia40b\n",
            "   Cambiando modelo activo a: alia40b\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   🏷️  Tipo: gguf\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   📝 Descripción: Modelo avanzado GGUF cuantizado\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "   Configuración cargada: mradermacher/ALIA-40b-GGUF\n",
            "\n",
            "🧠 Cargando modelo mradermacher/ALIA-40b-GGUF (modo optimizado)...\n",
            "📊 Configuración: 800 tokens máx, 0.5 temperatura\n",
            "🔧 Detectado modelo GGUF, cargando con llama-cpp...\n",
            "ALIA-40b.Q8_0.gguf: 100% 43.0G/43.0G [02:17<00:00, 313MB/s]\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100-SXM4-40GB) - 40004 MiB free\n",
            "llama_model_loader: loaded meta data with 40 key-value pairs and 435 tensors from models/models--mradermacher--ALIA-40b-GGUF/snapshots/5a4ef045b0a73ec820e86cea0f411d2378de285c/ALIA-40b.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = ALIA 40b\n",
            "llama_model_loader: - kv   3:                           general.basename str              = ALIA\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 40B\n",
            "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
            "llama_model_loader: - kv   7:                          general.languages arr[str,36]      = [\"bg\", \"ca\", \"code\", \"cs\", \"cy\", \"da\"...\n",
            "llama_model_loader: - kv   8:                          llama.block_count u32              = 48\n",
            "llama_model_loader: - kv   9:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 24576\n",
            "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 256000\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<|...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.add_space_prefix bool             = true\n",
            "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  32:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  33:                                general.url str              = https://huggingface.co/mradermacher/A...\n",
            "llama_model_loader: - kv  34:              mradermacher.quantize_version str              = 2\n",
            "llama_model_loader: - kv  35:                  mradermacher.quantized_by str              = mradermacher\n",
            "llama_model_loader: - kv  36:                  mradermacher.quantized_at str              = 2025-01-21T08:18:38+01:00\n",
            "llama_model_loader: - kv  37:                  mradermacher.quantized_on str              = marco\n",
            "llama_model_loader: - kv  38:                         general.source.url str              = https://huggingface.co/BSC-LT/ALIA-40b\n",
            "llama_model_loader: - kv  39:                  mradermacher.convert_type str              = hf\n",
            "llama_model_loader: - type  f32:   97 tensors\n",
            "llama_model_loader: - type q8_0:  338 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 40.01 GiB (8.50 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:     99 '<|reserved_token_94|>' is not marked as EOG\n",
            "load: control token:     79 '<|reserved_token_74|>' is not marked as EOG\n",
            "load: control token:     35 '<|reserved_token_30|>' is not marked as EOG\n",
            "load: control token:     44 '<|reserved_token_39|>' is not marked as EOG\n",
            "load: control token:     24 '<|reserved_token_19|>' is not marked as EOG\n",
            "load: control token:     29 '<|reserved_token_24|>' is not marked as EOG\n",
            "load: control token:     28 '<|reserved_token_23|>' is not marked as EOG\n",
            "load: control token:    101 '<|reserved_token_96|>' is not marked as EOG\n",
            "load: control token:     88 '<|reserved_token_83|>' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:     90 '<|reserved_token_85|>' is not marked as EOG\n",
            "load: control token:     87 '<|reserved_token_82|>' is not marked as EOG\n",
            "load: control token:      4 '<|im_start|>' is not marked as EOG\n",
            "load: control token:     86 '<|reserved_token_81|>' is not marked as EOG\n",
            "load: control token:     36 '<|reserved_token_31|>' is not marked as EOG\n",
            "load: control token:     38 '<|reserved_token_33|>' is not marked as EOG\n",
            "load: control token:     48 '<|reserved_token_43|>' is not marked as EOG\n",
            "load: control token:     89 '<|reserved_token_84|>' is not marked as EOG\n",
            "load: control token:      7 '<|reserved_token_2|>' is not marked as EOG\n",
            "load: control token:     40 '<|reserved_token_35|>' is not marked as EOG\n",
            "load: control token:     20 '<|reserved_token_15|>' is not marked as EOG\n",
            "load: control token:     91 '<|reserved_token_86|>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:     70 '<|reserved_token_65|>' is not marked as EOG\n",
            "load: control token:     75 '<|reserved_token_70|>' is not marked as EOG\n",
            "load: control token:     12 '<|reserved_token_7|>' is not marked as EOG\n",
            "load: control token:     52 '<|reserved_token_47|>' is not marked as EOG\n",
            "load: control token:     72 '<|reserved_token_67|>' is not marked as EOG\n",
            "load: control token:      9 '<|reserved_token_4|>' is not marked as EOG\n",
            "load: control token:     74 '<|reserved_token_69|>' is not marked as EOG\n",
            "load: control token:     94 '<|reserved_token_89|>' is not marked as EOG\n",
            "load: control token:     77 '<|reserved_token_72|>' is not marked as EOG\n",
            "load: control token:     76 '<|reserved_token_71|>' is not marked as EOG\n",
            "load: control token:     41 '<|reserved_token_36|>' is not marked as EOG\n",
            "load: control token:     42 '<|reserved_token_37|>' is not marked as EOG\n",
            "load: control token:     22 '<|reserved_token_17|>' is not marked as EOG\n",
            "load: control token:     32 '<|reserved_token_27|>' is not marked as EOG\n",
            "load: control token:     18 '<|reserved_token_13|>' is not marked as EOG\n",
            "load: control token:     66 '<|reserved_token_61|>' is not marked as EOG\n",
            "load: control token:     96 '<|reserved_token_91|>' is not marked as EOG\n",
            "load: control token:     93 '<|reserved_token_88|>' is not marked as EOG\n",
            "load: control token:     85 '<|reserved_token_80|>' is not marked as EOG\n",
            "load: control token:     47 '<|reserved_token_42|>' is not marked as EOG\n",
            "load: control token:     61 '<|reserved_token_56|>' is not marked as EOG\n",
            "load: control token:     71 '<|reserved_token_66|>' is not marked as EOG\n",
            "load: control token:     51 '<|reserved_token_46|>' is not marked as EOG\n",
            "load: control token:     43 '<|reserved_token_38|>' is not marked as EOG\n",
            "load: control token:      8 '<|reserved_token_3|>' is not marked as EOG\n",
            "load: control token:     14 '<|reserved_token_9|>' is not marked as EOG\n",
            "load: control token:     39 '<|reserved_token_34|>' is not marked as EOG\n",
            "load: control token:     68 '<|reserved_token_63|>' is not marked as EOG\n",
            "load: control token:     50 '<|reserved_token_45|>' is not marked as EOG\n",
            "load: control token:     46 '<|reserved_token_41|>' is not marked as EOG\n",
            "load: control token:     80 '<|reserved_token_75|>' is not marked as EOG\n",
            "load: control token:     56 '<|reserved_token_51|>' is not marked as EOG\n",
            "load: control token:      6 '<|reserved_token_1|>' is not marked as EOG\n",
            "load: control token:     84 '<|reserved_token_79|>' is not marked as EOG\n",
            "load: control token:     16 '<|reserved_token_11|>' is not marked as EOG\n",
            "load: control token:     17 '<|reserved_token_12|>' is not marked as EOG\n",
            "load: control token:     64 '<|reserved_token_59|>' is not marked as EOG\n",
            "load: control token:     83 '<|reserved_token_78|>' is not marked as EOG\n",
            "load: control token:      3 '<pad>' is not marked as EOG\n",
            "load: control token:     19 '<|reserved_token_14|>' is not marked as EOG\n",
            "load: control token:    102 '<|reserved_token_97|>' is not marked as EOG\n",
            "load: control token:     81 '<|reserved_token_76|>' is not marked as EOG\n",
            "load: control token:      0 '<unk>' is not marked as EOG\n",
            "load: control token:     23 '<|reserved_token_18|>' is not marked as EOG\n",
            "load: control token:     98 '<|reserved_token_93|>' is not marked as EOG\n",
            "load: control token:     15 '<|reserved_token_10|>' is not marked as EOG\n",
            "load: control token:     33 '<|reserved_token_28|>' is not marked as EOG\n",
            "load: control token:     60 '<|reserved_token_55|>' is not marked as EOG\n",
            "load: control token:     65 '<|reserved_token_60|>' is not marked as EOG\n",
            "load: control token:     78 '<|reserved_token_73|>' is not marked as EOG\n",
            "load: control token:     31 '<|reserved_token_26|>' is not marked as EOG\n",
            "load: control token:    103 '<|reserved_token_98|>' is not marked as EOG\n",
            "load: control token:     55 '<|reserved_token_50|>' is not marked as EOG\n",
            "load: control token:     82 '<|reserved_token_77|>' is not marked as EOG\n",
            "load: control token:     63 '<|reserved_token_58|>' is not marked as EOG\n",
            "load: control token:     54 '<|reserved_token_49|>' is not marked as EOG\n",
            "load: control token:     26 '<|reserved_token_21|>' is not marked as EOG\n",
            "load: control token:     73 '<|reserved_token_68|>' is not marked as EOG\n",
            "load: control token:     25 '<|reserved_token_20|>' is not marked as EOG\n",
            "load: control token:     45 '<|reserved_token_40|>' is not marked as EOG\n",
            "load: control token:     30 '<|reserved_token_25|>' is not marked as EOG\n",
            "load: control token:     10 '<|reserved_token_5|>' is not marked as EOG\n",
            "load: control token:     49 '<|reserved_token_44|>' is not marked as EOG\n",
            "load: control token:     27 '<|reserved_token_22|>' is not marked as EOG\n",
            "load: control token:     67 '<|reserved_token_62|>' is not marked as EOG\n",
            "load: control token:     13 '<|reserved_token_8|>' is not marked as EOG\n",
            "load: control token:     57 '<|reserved_token_52|>' is not marked as EOG\n",
            "load: control token:     37 '<|reserved_token_32|>' is not marked as EOG\n",
            "load: control token:     59 '<|reserved_token_54|>' is not marked as EOG\n",
            "load: control token:     58 '<|reserved_token_53|>' is not marked as EOG\n",
            "load: control token:     11 '<|reserved_token_6|>' is not marked as EOG\n",
            "load: control token:     97 '<|reserved_token_92|>' is not marked as EOG\n",
            "load: control token:     69 '<|reserved_token_64|>' is not marked as EOG\n",
            "load: control token:     92 '<|reserved_token_87|>' is not marked as EOG\n",
            "load: control token:     62 '<|reserved_token_57|>' is not marked as EOG\n",
            "load: control token:     53 '<|reserved_token_48|>' is not marked as EOG\n",
            "load: control token:     95 '<|reserved_token_90|>' is not marked as EOG\n",
            "load: control token:     21 '<|reserved_token_16|>' is not marked as EOG\n",
            "load: control token:    100 '<|reserved_token_95|>' is not marked as EOG\n",
            "load: control token:     34 '<|reserved_token_29|>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load:   - 5 ('<|im_end|>')\n",
            "load: special tokens cache size = 104\n",
            "load: token to piece cache size = 1.8842 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 8192\n",
            "print_info: n_layer          = 48\n",
            "print_info: n_head           = 64\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 24576\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 34B\n",
            "print_info: model params     = 40.43 B\n",
            "print_info: general.name     = ALIA 40b\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 256000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: EOT token        = 5 '<|im_end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 145 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: EOG token        = 5 '<|im_end|>'\n",
            "print_info: max token length = 72\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  33 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  34 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  35 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  36 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  37 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  38 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  39 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  40 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  41 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  42 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  43 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  44 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  45 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  46 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  47 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  48 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
            "load_tensors: offloading 48 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 49/49 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size = 38848.03 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  2125.00 MiB\n",
            "...........................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.98 MiB\n",
            "create_memory: n_ctx = 2048 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  32: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  33: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  34: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  35: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  36: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  37: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  38: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  39: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  40: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  41: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  42: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  43: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  44: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  45: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  46: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  47: dev = CUDA0\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =   384.00 MiB\n",
            "llama_kv_cache_unified: size =  384.00 MiB (  2048 cells,  48 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 3480\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:      CUDA0 compute buffer size =   516.00 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_context: graph nodes  = 1686\n",
            "llama_context: graph splits = 2\n",
            "CUDA : ARCHS = 800 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'mradermacher.quantized_on': 'marco', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantize_version': '2', 'llama.attention.head_count_kv': '8', 'mradermacher.quantized_at': '2025-01-21T08:18:38+01:00', 'llama.embedding_length': '8192', 'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/BSC-LT/ALIA-40b', 'llama.feed_forward_length': '24576', 'general.license': 'apache-2.0', 'llama.attention.value_length': '128', 'tokenizer.ggml.add_bos_token': 'true', 'general.size_label': '40B', 'general.type': 'model', 'llama.context_length': '4096', 'general.name': 'ALIA 40b', 'tokenizer.ggml.bos_token_id': '1', 'general.basename': 'ALIA', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'general.url': 'https://huggingface.co/mradermacher/ALIA-40b-GGUF', 'llama.block_count': '48', 'llama.attention.head_count': '64', 'llama.attention.key_length': '128', 'tokenizer.ggml.pre': 'default', 'llama.vocab_size': '256000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_eos_token': 'false', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.add_space_prefix': 'true', 'general.file_type': '7'}\n",
            "Using fallback chat format: llama-2\n",
            "✅ Modelo GGUF cargado\n",
            "✅ Modelo ALIA 40B GGUF cargado en modo optimizado\n",
            "\n",
            "🔄 INTENTANDO CAMBIAR MODELO A: alia40b\n",
            "\n",
            "🔄 SOLICITUD DE CAMBIO DE MODELO: alia40b\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   🏷️  Tipo: gguf\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   📝 Descripción: Modelo avanzado GGUF cuantizado\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "🔧 Recargando motor de chat...\n",
            "🔄 Cambiando de modelo: liberando anterior...\n",
            "🧹 INICIANDO LIBERACIÓN COMPLETA DE VRAM...\n",
            "🔧 Liberando modelo GGUF (llama-cpp)...\n",
            "🔥 Limpieza agresiva CUDA...\n",
            "✅ VRAM liberada: 0.0GB / 0.0GB\n",
            "✅ Modelo completamente descargado\n",
            "\n",
            "🧠 INICIALIZANDO CHAT ENGINE\n",
            "   Modelo solicitado: alia40b\n",
            "   Cambiando modelo activo a: alia40b\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   🏷️  Tipo: gguf\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   📝 Descripción: Modelo avanzado GGUF cuantizado\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "   Configuración cargada: mradermacher/ALIA-40b-GGUF\n",
            "\n",
            "🧠 Cargando modelo mradermacher/ALIA-40b-GGUF (modo optimizado)...\n",
            "📊 Configuración: 800 tokens máx, 0.5 temperatura\n",
            "🔧 Detectado modelo GGUF, cargando con llama-cpp...\n",
            "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100-SXM4-40GB) - 40004 MiB free\n",
            "llama_model_loader: loaded meta data with 40 key-value pairs and 435 tensors from models/models--mradermacher--ALIA-40b-GGUF/snapshots/5a4ef045b0a73ec820e86cea0f411d2378de285c/ALIA-40b.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = ALIA 40b\n",
            "llama_model_loader: - kv   3:                           general.basename str              = ALIA\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 40B\n",
            "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
            "llama_model_loader: - kv   7:                          general.languages arr[str,36]      = [\"bg\", \"ca\", \"code\", \"cs\", \"cy\", \"da\"...\n",
            "llama_model_loader: - kv   8:                          llama.block_count u32              = 48\n",
            "llama_model_loader: - kv   9:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 24576\n",
            "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 256000\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<|...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.add_space_prefix bool             = true\n",
            "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  32:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  33:                                general.url str              = https://huggingface.co/mradermacher/A...\n",
            "llama_model_loader: - kv  34:              mradermacher.quantize_version str              = 2\n",
            "llama_model_loader: - kv  35:                  mradermacher.quantized_by str              = mradermacher\n",
            "llama_model_loader: - kv  36:                  mradermacher.quantized_at str              = 2025-01-21T08:18:38+01:00\n",
            "llama_model_loader: - kv  37:                  mradermacher.quantized_on str              = marco\n",
            "llama_model_loader: - kv  38:                         general.source.url str              = https://huggingface.co/BSC-LT/ALIA-40b\n",
            "llama_model_loader: - kv  39:                  mradermacher.convert_type str              = hf\n",
            "llama_model_loader: - type  f32:   97 tensors\n",
            "llama_model_loader: - type q8_0:  338 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 40.01 GiB (8.50 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:     99 '<|reserved_token_94|>' is not marked as EOG\n",
            "load: control token:     79 '<|reserved_token_74|>' is not marked as EOG\n",
            "load: control token:     35 '<|reserved_token_30|>' is not marked as EOG\n",
            "load: control token:     44 '<|reserved_token_39|>' is not marked as EOG\n",
            "load: control token:     24 '<|reserved_token_19|>' is not marked as EOG\n",
            "load: control token:     29 '<|reserved_token_24|>' is not marked as EOG\n",
            "load: control token:     28 '<|reserved_token_23|>' is not marked as EOG\n",
            "load: control token:    101 '<|reserved_token_96|>' is not marked as EOG\n",
            "load: control token:     88 '<|reserved_token_83|>' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:     90 '<|reserved_token_85|>' is not marked as EOG\n",
            "load: control token:     87 '<|reserved_token_82|>' is not marked as EOG\n",
            "load: control token:      4 '<|im_start|>' is not marked as EOG\n",
            "load: control token:     86 '<|reserved_token_81|>' is not marked as EOG\n",
            "load: control token:     36 '<|reserved_token_31|>' is not marked as EOG\n",
            "load: control token:     38 '<|reserved_token_33|>' is not marked as EOG\n",
            "load: control token:     48 '<|reserved_token_43|>' is not marked as EOG\n",
            "load: control token:     89 '<|reserved_token_84|>' is not marked as EOG\n",
            "load: control token:      7 '<|reserved_token_2|>' is not marked as EOG\n",
            "load: control token:     40 '<|reserved_token_35|>' is not marked as EOG\n",
            "load: control token:     20 '<|reserved_token_15|>' is not marked as EOG\n",
            "load: control token:     91 '<|reserved_token_86|>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:     70 '<|reserved_token_65|>' is not marked as EOG\n",
            "load: control token:     75 '<|reserved_token_70|>' is not marked as EOG\n",
            "load: control token:     12 '<|reserved_token_7|>' is not marked as EOG\n",
            "load: control token:     52 '<|reserved_token_47|>' is not marked as EOG\n",
            "load: control token:     72 '<|reserved_token_67|>' is not marked as EOG\n",
            "load: control token:      9 '<|reserved_token_4|>' is not marked as EOG\n",
            "load: control token:     74 '<|reserved_token_69|>' is not marked as EOG\n",
            "load: control token:     94 '<|reserved_token_89|>' is not marked as EOG\n",
            "load: control token:     77 '<|reserved_token_72|>' is not marked as EOG\n",
            "load: control token:     76 '<|reserved_token_71|>' is not marked as EOG\n",
            "load: control token:     41 '<|reserved_token_36|>' is not marked as EOG\n",
            "load: control token:     42 '<|reserved_token_37|>' is not marked as EOG\n",
            "load: control token:     22 '<|reserved_token_17|>' is not marked as EOG\n",
            "load: control token:     32 '<|reserved_token_27|>' is not marked as EOG\n",
            "load: control token:     18 '<|reserved_token_13|>' is not marked as EOG\n",
            "load: control token:     66 '<|reserved_token_61|>' is not marked as EOG\n",
            "load: control token:     96 '<|reserved_token_91|>' is not marked as EOG\n",
            "load: control token:     93 '<|reserved_token_88|>' is not marked as EOG\n",
            "load: control token:     85 '<|reserved_token_80|>' is not marked as EOG\n",
            "load: control token:     47 '<|reserved_token_42|>' is not marked as EOG\n",
            "load: control token:     61 '<|reserved_token_56|>' is not marked as EOG\n",
            "load: control token:     71 '<|reserved_token_66|>' is not marked as EOG\n",
            "load: control token:     51 '<|reserved_token_46|>' is not marked as EOG\n",
            "load: control token:     43 '<|reserved_token_38|>' is not marked as EOG\n",
            "load: control token:      8 '<|reserved_token_3|>' is not marked as EOG\n",
            "load: control token:     14 '<|reserved_token_9|>' is not marked as EOG\n",
            "load: control token:     39 '<|reserved_token_34|>' is not marked as EOG\n",
            "load: control token:     68 '<|reserved_token_63|>' is not marked as EOG\n",
            "load: control token:     50 '<|reserved_token_45|>' is not marked as EOG\n",
            "load: control token:     46 '<|reserved_token_41|>' is not marked as EOG\n",
            "load: control token:     80 '<|reserved_token_75|>' is not marked as EOG\n",
            "load: control token:     56 '<|reserved_token_51|>' is not marked as EOG\n",
            "load: control token:      6 '<|reserved_token_1|>' is not marked as EOG\n",
            "load: control token:     84 '<|reserved_token_79|>' is not marked as EOG\n",
            "load: control token:     16 '<|reserved_token_11|>' is not marked as EOG\n",
            "load: control token:     17 '<|reserved_token_12|>' is not marked as EOG\n",
            "load: control token:     64 '<|reserved_token_59|>' is not marked as EOG\n",
            "load: control token:     83 '<|reserved_token_78|>' is not marked as EOG\n",
            "load: control token:      3 '<pad>' is not marked as EOG\n",
            "load: control token:     19 '<|reserved_token_14|>' is not marked as EOG\n",
            "load: control token:    102 '<|reserved_token_97|>' is not marked as EOG\n",
            "load: control token:     81 '<|reserved_token_76|>' is not marked as EOG\n",
            "load: control token:      0 '<unk>' is not marked as EOG\n",
            "load: control token:     23 '<|reserved_token_18|>' is not marked as EOG\n",
            "load: control token:     98 '<|reserved_token_93|>' is not marked as EOG\n",
            "load: control token:     15 '<|reserved_token_10|>' is not marked as EOG\n",
            "load: control token:     33 '<|reserved_token_28|>' is not marked as EOG\n",
            "load: control token:     60 '<|reserved_token_55|>' is not marked as EOG\n",
            "load: control token:     65 '<|reserved_token_60|>' is not marked as EOG\n",
            "load: control token:     78 '<|reserved_token_73|>' is not marked as EOG\n",
            "load: control token:     31 '<|reserved_token_26|>' is not marked as EOG\n",
            "load: control token:    103 '<|reserved_token_98|>' is not marked as EOG\n",
            "load: control token:     55 '<|reserved_token_50|>' is not marked as EOG\n",
            "load: control token:     82 '<|reserved_token_77|>' is not marked as EOG\n",
            "load: control token:     63 '<|reserved_token_58|>' is not marked as EOG\n",
            "load: control token:     54 '<|reserved_token_49|>' is not marked as EOG\n",
            "load: control token:     26 '<|reserved_token_21|>' is not marked as EOG\n",
            "load: control token:     73 '<|reserved_token_68|>' is not marked as EOG\n",
            "load: control token:     25 '<|reserved_token_20|>' is not marked as EOG\n",
            "load: control token:     45 '<|reserved_token_40|>' is not marked as EOG\n",
            "load: control token:     30 '<|reserved_token_25|>' is not marked as EOG\n",
            "load: control token:     10 '<|reserved_token_5|>' is not marked as EOG\n",
            "load: control token:     49 '<|reserved_token_44|>' is not marked as EOG\n",
            "load: control token:     27 '<|reserved_token_22|>' is not marked as EOG\n",
            "load: control token:     67 '<|reserved_token_62|>' is not marked as EOG\n",
            "load: control token:     13 '<|reserved_token_8|>' is not marked as EOG\n",
            "load: control token:     57 '<|reserved_token_52|>' is not marked as EOG\n",
            "load: control token:     37 '<|reserved_token_32|>' is not marked as EOG\n",
            "load: control token:     59 '<|reserved_token_54|>' is not marked as EOG\n",
            "load: control token:     58 '<|reserved_token_53|>' is not marked as EOG\n",
            "load: control token:     11 '<|reserved_token_6|>' is not marked as EOG\n",
            "load: control token:     97 '<|reserved_token_92|>' is not marked as EOG\n",
            "load: control token:     69 '<|reserved_token_64|>' is not marked as EOG\n",
            "load: control token:     92 '<|reserved_token_87|>' is not marked as EOG\n",
            "load: control token:     62 '<|reserved_token_57|>' is not marked as EOG\n",
            "load: control token:     53 '<|reserved_token_48|>' is not marked as EOG\n",
            "load: control token:     95 '<|reserved_token_90|>' is not marked as EOG\n",
            "load: control token:     21 '<|reserved_token_16|>' is not marked as EOG\n",
            "load: control token:    100 '<|reserved_token_95|>' is not marked as EOG\n",
            "load: control token:     34 '<|reserved_token_29|>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load:   - 5 ('<|im_end|>')\n",
            "load: special tokens cache size = 104\n",
            "load: token to piece cache size = 1.8842 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 8192\n",
            "print_info: n_layer          = 48\n",
            "print_info: n_head           = 64\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 24576\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 34B\n",
            "print_info: model params     = 40.43 B\n",
            "print_info: general.name     = ALIA 40b\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 256000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: EOT token        = 5 '<|im_end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 145 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: EOG token        = 5 '<|im_end|>'\n",
            "print_info: max token length = 72\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  33 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  34 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  35 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  36 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  37 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  38 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  39 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  40 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  41 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  42 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  43 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  44 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  45 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  46 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  47 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  48 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
            "load_tensors: offloading 48 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 49/49 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size = 38848.03 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  2125.00 MiB\n",
            "...........................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.98 MiB\n",
            "create_memory: n_ctx = 2048 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  32: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  33: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  34: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  35: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  36: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  37: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  38: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  39: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  40: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  41: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  42: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  43: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  44: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  45: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  46: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  47: dev = CUDA0\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =   384.00 MiB\n",
            "llama_kv_cache_unified: size =  384.00 MiB (  2048 cells,  48 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 3480\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:      CUDA0 compute buffer size =   516.00 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_context: graph nodes  = 1686\n",
            "llama_context: graph splits = 2\n",
            "CUDA : ARCHS = 800 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'mradermacher.quantized_on': 'marco', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantize_version': '2', 'llama.attention.head_count_kv': '8', 'mradermacher.quantized_at': '2025-01-21T08:18:38+01:00', 'llama.embedding_length': '8192', 'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/BSC-LT/ALIA-40b', 'llama.feed_forward_length': '24576', 'general.license': 'apache-2.0', 'llama.attention.value_length': '128', 'tokenizer.ggml.add_bos_token': 'true', 'general.size_label': '40B', 'general.type': 'model', 'llama.context_length': '4096', 'general.name': 'ALIA 40b', 'tokenizer.ggml.bos_token_id': '1', 'general.basename': 'ALIA', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'general.url': 'https://huggingface.co/mradermacher/ALIA-40b-GGUF', 'llama.block_count': '48', 'llama.attention.head_count': '64', 'llama.attention.key_length': '128', 'tokenizer.ggml.pre': 'default', 'llama.vocab_size': '256000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_eos_token': 'false', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.add_space_prefix': 'true', 'general.file_type': '7'}\n",
            "Using fallback chat format: llama-2\n",
            "✅ Modelo GGUF cargado\n",
            "✅ Modelo ALIA 40B GGUF cargado en modo optimizado\n",
            "   ✅ Cambio exitoso: ✅ Modelo cambiado exitosamente a: ALIA 40B GGUF\n",
            "\n",
            "============================================================\n",
            "💬 CHAT: 'que opinas de los esclavos que escapaban de las trece colonias a territorios esp...'\n",
            "\n",
            "🔍 CONSULTA: 'que opinas de los esclavos que escapaban de las trece colonias a territorios esp...'\n",
            "🤖 Modelo: ALIA 40B GGUF\n",
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100% 79.3M/79.3M [00:08<00:00, 9.62MiB/s]\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
            "   📚 Documentos relevantes encontrados: 3\n",
            "llama_perf_context_print:        load time =     714.27 ms\n",
            "llama_perf_context_print: prompt eval time =     713.86 ms /   203 tokens (    3.52 ms per token,   284.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3654.34 ms /   103 runs   (   35.48 ms per token,    28.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    4600.93 ms /   306 tokens\n",
            "llama_perf_context_print:    graphs reused =         99\n",
            "✅ Respuesta en 4.6s, 478 caracteres (Modelo: ALIA 40B GGUF)\n",
            "   ✅ Respuesta generada\n",
            "🔍 Modelo seleccionado: salamandra7b\n",
            "🧹 INICIANDO LIBERACIÓN COMPLETA DE VRAM...\n",
            "🔧 Liberando modelo GGUF (llama-cpp)...\n",
            "🔥 Limpieza agresiva CUDA...\n",
            "✅ VRAM liberada: 0.0GB / 0.0GB\n",
            "✅ Modelo completamente descargado\n",
            "✅ Modelo cambiado a: Salamandra 7B\n",
            "   🏷️  Tipo: transformers\n",
            "   💾 Memoria requerida: 4-6 GB\n",
            "✅ Modelo cambiado a: Salamandra 7B\n",
            "   📝 Descripción: Modelo equilibrado (por defecto)\n",
            "   💾 Memoria requerida: 4-6 GB\n",
            "🔄 Cambiando de modelo: liberando anterior...\n",
            "🧹 INICIANDO LIBERACIÓN COMPLETA DE VRAM...\n",
            "🔥 Limpieza agresiva CUDA...\n",
            "✅ VRAM liberada: 0.0GB / 0.0GB\n",
            "✅ Modelo completamente descargado\n",
            "\n",
            "🧠 INICIALIZANDO CHAT ENGINE\n",
            "   Modelo solicitado: salamandra7b\n",
            "   Cambiando modelo activo a: salamandra7b\n",
            "✅ Modelo cambiado a: Salamandra 7B\n",
            "   🏷️  Tipo: transformers\n",
            "   💾 Memoria requerida: 4-6 GB\n",
            "✅ Modelo cambiado a: Salamandra 7B\n",
            "   📝 Descripción: Modelo equilibrado (por defecto)\n",
            "   💾 Memoria requerida: 4-6 GB\n",
            "   Configuración cargada: BSC-LT/salamandra-7b-instruct\n",
            "\n",
            "🧠 Cargando modelo BSC-LT/salamandra-7b-instruct (modo optimizado)...\n",
            "📊 Configuración: 600 tokens máx, 0.5 temperatura\n",
            "Loading checkpoint shards: 100% 4/4 [01:22<00:00, 20.51s/it]\n",
            "✅ Modelo Salamandra 7B cargado en modo optimizado\n",
            "\n",
            "🔄 INTENTANDO CAMBIAR MODELO A: salamandra7b\n",
            "\n",
            "🔄 SOLICITUD DE CAMBIO DE MODELO: salamandra7b\n",
            "✅ Modelo cambiado a: Salamandra 7B\n",
            "   🏷️  Tipo: transformers\n",
            "   💾 Memoria requerida: 4-6 GB\n",
            "✅ Modelo cambiado a: Salamandra 7B\n",
            "   📝 Descripción: Modelo equilibrado (por defecto)\n",
            "   💾 Memoria requerida: 4-6 GB\n",
            "🔧 Recargando motor de chat...\n",
            "🔄 Cambiando de modelo: liberando anterior...\n",
            "🧹 INICIANDO LIBERACIÓN COMPLETA DE VRAM...\n",
            "🔥 Limpieza agresiva CUDA...\n",
            "✅ VRAM liberada: 0.0GB / 0.0GB\n",
            "✅ Modelo completamente descargado\n",
            "\n",
            "🧠 INICIALIZANDO CHAT ENGINE\n",
            "   Modelo solicitado: salamandra7b\n",
            "   Cambiando modelo activo a: salamandra7b\n",
            "✅ Modelo cambiado a: Salamandra 7B\n",
            "   🏷️  Tipo: transformers\n",
            "   💾 Memoria requerida: 4-6 GB\n",
            "✅ Modelo cambiado a: Salamandra 7B\n",
            "   📝 Descripción: Modelo equilibrado (por defecto)\n",
            "   💾 Memoria requerida: 4-6 GB\n",
            "   Configuración cargada: BSC-LT/salamandra-7b-instruct\n",
            "\n",
            "🧠 Cargando modelo BSC-LT/salamandra-7b-instruct (modo optimizado)...\n",
            "📊 Configuración: 600 tokens máx, 0.5 temperatura\n",
            "Loading checkpoint shards: 100% 4/4 [00:15<00:00,  3.80s/it]\n",
            "✅ Modelo Salamandra 7B cargado en modo optimizado\n",
            "   ✅ Cambio exitoso: ✅ Modelo cambiado exitosamente a: Salamandra 7B\n",
            "\n",
            "============================================================\n",
            "💬 CHAT: 'que opinas de los esclavos que escapaban de las trece colonias a territorios esp...'\n",
            "\n",
            "🔍 CONSULTA: 'que opinas de los esclavos que escapaban de las trece colonias a territorios esp...'\n",
            "🤖 Modelo: Salamandra 7B\n",
            "   📚 Documentos relevantes encontrados: 3\n",
            "✅ Respuesta en 42.5s, 3873 caracteres (Modelo: Salamandra 7B)\n",
            "   ✅ Respuesta generada\n",
            "🔍 Modelo seleccionado: alia40b\n",
            "🧹 INICIANDO LIBERACIÓN COMPLETA DE VRAM...\n",
            "🔥 Limpieza agresiva CUDA...\n",
            "✅ VRAM liberada: 0.0GB / 6.7GB\n",
            "✅ Modelo completamente descargado\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   🏷️  Tipo: gguf\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   📝 Descripción: Modelo avanzado GGUF cuantizado\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "🔄 Cambiando de modelo: liberando anterior...\n",
            "🧹 INICIANDO LIBERACIÓN COMPLETA DE VRAM...\n",
            "🔥 Limpieza agresiva CUDA...\n",
            "✅ VRAM liberada: 0.0GB / 6.7GB\n",
            "✅ Modelo completamente descargado\n",
            "\n",
            "🧠 INICIALIZANDO CHAT ENGINE\n",
            "   Modelo solicitado: alia40b\n",
            "   Cambiando modelo activo a: alia40b\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   🏷️  Tipo: gguf\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "✅ Modelo cambiado a: ALIA 40B GGUF\n",
            "   📝 Descripción: Modelo avanzado GGUF cuantizado\n",
            "   💾 Memoria requerida: 15-18 GB\n",
            "   Configuración cargada: mradermacher/ALIA-40b-GGUF\n",
            "\n",
            "🧠 Cargando modelo mradermacher/ALIA-40b-GGUF (modo optimizado)...\n",
            "📊 Configuración: 800 tokens máx, 0.5 temperatura\n",
            "🔧 Detectado modelo GGUF, cargando con llama-cpp...\n",
            "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100-SXM4-40GB) - 33560 MiB free\n",
            "llama_model_loader: loaded meta data with 40 key-value pairs and 435 tensors from models/models--mradermacher--ALIA-40b-GGUF/snapshots/5a4ef045b0a73ec820e86cea0f411d2378de285c/ALIA-40b.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = ALIA 40b\n",
            "llama_model_loader: - kv   3:                           general.basename str              = ALIA\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 40B\n",
            "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
            "llama_model_loader: - kv   7:                          general.languages arr[str,36]      = [\"bg\", \"ca\", \"code\", \"cs\", \"cy\", \"da\"...\n",
            "llama_model_loader: - kv   8:                          llama.block_count u32              = 48\n",
            "llama_model_loader: - kv   9:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 24576\n",
            "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 256000\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<|...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.add_space_prefix bool             = true\n",
            "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  32:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  33:                                general.url str              = https://huggingface.co/mradermacher/A...\n",
            "llama_model_loader: - kv  34:              mradermacher.quantize_version str              = 2\n",
            "llama_model_loader: - kv  35:                  mradermacher.quantized_by str              = mradermacher\n",
            "llama_model_loader: - kv  36:                  mradermacher.quantized_at str              = 2025-01-21T08:18:38+01:00\n",
            "llama_model_loader: - kv  37:                  mradermacher.quantized_on str              = marco\n",
            "llama_model_loader: - kv  38:                         general.source.url str              = https://huggingface.co/BSC-LT/ALIA-40b\n",
            "llama_model_loader: - kv  39:                  mradermacher.convert_type str              = hf\n",
            "llama_model_loader: - type  f32:   97 tensors\n",
            "llama_model_loader: - type q8_0:  338 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 40.01 GiB (8.50 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:     99 '<|reserved_token_94|>' is not marked as EOG\n",
            "load: control token:     79 '<|reserved_token_74|>' is not marked as EOG\n",
            "load: control token:     35 '<|reserved_token_30|>' is not marked as EOG\n",
            "load: control token:     44 '<|reserved_token_39|>' is not marked as EOG\n",
            "load: control token:     24 '<|reserved_token_19|>' is not marked as EOG\n",
            "load: control token:     29 '<|reserved_token_24|>' is not marked as EOG\n",
            "load: control token:     28 '<|reserved_token_23|>' is not marked as EOG\n",
            "load: control token:    101 '<|reserved_token_96|>' is not marked as EOG\n",
            "load: control token:     88 '<|reserved_token_83|>' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:     90 '<|reserved_token_85|>' is not marked as EOG\n",
            "load: control token:     87 '<|reserved_token_82|>' is not marked as EOG\n",
            "load: control token:      4 '<|im_start|>' is not marked as EOG\n",
            "load: control token:     86 '<|reserved_token_81|>' is not marked as EOG\n",
            "load: control token:     36 '<|reserved_token_31|>' is not marked as EOG\n",
            "load: control token:     38 '<|reserved_token_33|>' is not marked as EOG\n",
            "load: control token:     48 '<|reserved_token_43|>' is not marked as EOG\n",
            "load: control token:     89 '<|reserved_token_84|>' is not marked as EOG\n",
            "load: control token:      7 '<|reserved_token_2|>' is not marked as EOG\n",
            "load: control token:     40 '<|reserved_token_35|>' is not marked as EOG\n",
            "load: control token:     20 '<|reserved_token_15|>' is not marked as EOG\n",
            "load: control token:     91 '<|reserved_token_86|>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:     70 '<|reserved_token_65|>' is not marked as EOG\n",
            "load: control token:     75 '<|reserved_token_70|>' is not marked as EOG\n",
            "load: control token:     12 '<|reserved_token_7|>' is not marked as EOG\n",
            "load: control token:     52 '<|reserved_token_47|>' is not marked as EOG\n",
            "load: control token:     72 '<|reserved_token_67|>' is not marked as EOG\n",
            "load: control token:      9 '<|reserved_token_4|>' is not marked as EOG\n",
            "load: control token:     74 '<|reserved_token_69|>' is not marked as EOG\n",
            "load: control token:     94 '<|reserved_token_89|>' is not marked as EOG\n",
            "load: control token:     77 '<|reserved_token_72|>' is not marked as EOG\n",
            "load: control token:     76 '<|reserved_token_71|>' is not marked as EOG\n",
            "load: control token:     41 '<|reserved_token_36|>' is not marked as EOG\n",
            "load: control token:     42 '<|reserved_token_37|>' is not marked as EOG\n",
            "load: control token:     22 '<|reserved_token_17|>' is not marked as EOG\n",
            "load: control token:     32 '<|reserved_token_27|>' is not marked as EOG\n",
            "load: control token:     18 '<|reserved_token_13|>' is not marked as EOG\n",
            "load: control token:     66 '<|reserved_token_61|>' is not marked as EOG\n",
            "load: control token:     96 '<|reserved_token_91|>' is not marked as EOG\n",
            "load: control token:     93 '<|reserved_token_88|>' is not marked as EOG\n",
            "load: control token:     85 '<|reserved_token_80|>' is not marked as EOG\n",
            "load: control token:     47 '<|reserved_token_42|>' is not marked as EOG\n",
            "load: control token:     61 '<|reserved_token_56|>' is not marked as EOG\n",
            "load: control token:     71 '<|reserved_token_66|>' is not marked as EOG\n",
            "load: control token:     51 '<|reserved_token_46|>' is not marked as EOG\n",
            "load: control token:     43 '<|reserved_token_38|>' is not marked as EOG\n",
            "load: control token:      8 '<|reserved_token_3|>' is not marked as EOG\n",
            "load: control token:     14 '<|reserved_token_9|>' is not marked as EOG\n",
            "load: control token:     39 '<|reserved_token_34|>' is not marked as EOG\n",
            "load: control token:     68 '<|reserved_token_63|>' is not marked as EOG\n",
            "load: control token:     50 '<|reserved_token_45|>' is not marked as EOG\n",
            "load: control token:     46 '<|reserved_token_41|>' is not marked as EOG\n",
            "load: control token:     80 '<|reserved_token_75|>' is not marked as EOG\n",
            "load: control token:     56 '<|reserved_token_51|>' is not marked as EOG\n",
            "load: control token:      6 '<|reserved_token_1|>' is not marked as EOG\n",
            "load: control token:     84 '<|reserved_token_79|>' is not marked as EOG\n",
            "load: control token:     16 '<|reserved_token_11|>' is not marked as EOG\n",
            "load: control token:     17 '<|reserved_token_12|>' is not marked as EOG\n",
            "load: control token:     64 '<|reserved_token_59|>' is not marked as EOG\n",
            "load: control token:     83 '<|reserved_token_78|>' is not marked as EOG\n",
            "load: control token:      3 '<pad>' is not marked as EOG\n",
            "load: control token:     19 '<|reserved_token_14|>' is not marked as EOG\n",
            "load: control token:    102 '<|reserved_token_97|>' is not marked as EOG\n",
            "load: control token:     81 '<|reserved_token_76|>' is not marked as EOG\n",
            "load: control token:      0 '<unk>' is not marked as EOG\n",
            "load: control token:     23 '<|reserved_token_18|>' is not marked as EOG\n",
            "load: control token:     98 '<|reserved_token_93|>' is not marked as EOG\n",
            "load: control token:     15 '<|reserved_token_10|>' is not marked as EOG\n",
            "load: control token:     33 '<|reserved_token_28|>' is not marked as EOG\n",
            "load: control token:     60 '<|reserved_token_55|>' is not marked as EOG\n",
            "load: control token:     65 '<|reserved_token_60|>' is not marked as EOG\n",
            "load: control token:     78 '<|reserved_token_73|>' is not marked as EOG\n",
            "load: control token:     31 '<|reserved_token_26|>' is not marked as EOG\n",
            "load: control token:    103 '<|reserved_token_98|>' is not marked as EOG\n",
            "load: control token:     55 '<|reserved_token_50|>' is not marked as EOG\n",
            "load: control token:     82 '<|reserved_token_77|>' is not marked as EOG\n",
            "load: control token:     63 '<|reserved_token_58|>' is not marked as EOG\n",
            "load: control token:     54 '<|reserved_token_49|>' is not marked as EOG\n",
            "load: control token:     26 '<|reserved_token_21|>' is not marked as EOG\n",
            "load: control token:     73 '<|reserved_token_68|>' is not marked as EOG\n",
            "load: control token:     25 '<|reserved_token_20|>' is not marked as EOG\n",
            "load: control token:     45 '<|reserved_token_40|>' is not marked as EOG\n",
            "load: control token:     30 '<|reserved_token_25|>' is not marked as EOG\n",
            "load: control token:     10 '<|reserved_token_5|>' is not marked as EOG\n",
            "load: control token:     49 '<|reserved_token_44|>' is not marked as EOG\n",
            "load: control token:     27 '<|reserved_token_22|>' is not marked as EOG\n",
            "load: control token:     67 '<|reserved_token_62|>' is not marked as EOG\n",
            "load: control token:     13 '<|reserved_token_8|>' is not marked as EOG\n",
            "load: control token:     57 '<|reserved_token_52|>' is not marked as EOG\n",
            "load: control token:     37 '<|reserved_token_32|>' is not marked as EOG\n",
            "load: control token:     59 '<|reserved_token_54|>' is not marked as EOG\n",
            "load: control token:     58 '<|reserved_token_53|>' is not marked as EOG\n",
            "load: control token:     11 '<|reserved_token_6|>' is not marked as EOG\n",
            "load: control token:     97 '<|reserved_token_92|>' is not marked as EOG\n",
            "load: control token:     69 '<|reserved_token_64|>' is not marked as EOG\n",
            "load: control token:     92 '<|reserved_token_87|>' is not marked as EOG\n",
            "load: control token:     62 '<|reserved_token_57|>' is not marked as EOG\n",
            "load: control token:     53 '<|reserved_token_48|>' is not marked as EOG\n",
            "load: control token:     95 '<|reserved_token_90|>' is not marked as EOG\n",
            "load: control token:     21 '<|reserved_token_16|>' is not marked as EOG\n",
            "load: control token:    100 '<|reserved_token_95|>' is not marked as EOG\n",
            "load: control token:     34 '<|reserved_token_29|>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load:   - 5 ('<|im_end|>')\n",
            "load: special tokens cache size = 104\n",
            "load: token to piece cache size = 1.8842 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 8192\n",
            "print_info: n_layer          = 48\n",
            "print_info: n_head           = 64\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 24576\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 34B\n",
            "print_info: model params     = 40.43 B\n",
            "print_info: general.name     = ALIA 40b\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 256000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: EOT token        = 5 '<|im_end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 145 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: EOG token        = 5 '<|im_end|>'\n",
            "print_info: max token length = 72\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  33 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  34 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  35 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  36 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  37 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  38 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  39 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  40 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  41 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  42 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  43 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  44 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  45 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  46 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  47 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  48 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
            "ggml_backend_cuda_buffer_type_alloc_buffer: allocating 38848.03 MiB on device 0: cudaMalloc failed: out of memory\n",
            "alloc_tensor_range: failed to allocate CUDA0 buffer of size 40735113216\n",
            "llama_model_load: error loading model: unable to allocate CUDA0 buffer\n",
            "llama_model_load_from_file_impl: failed to load model\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 489, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1561, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1179, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 61, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 678, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ALIA-ChatBot/src/interface/gradio_interface.py\", line 297, in _change_model\n",
            "    self.orchestrator.reload_llm(model_key)\n",
            "  File \"/content/ALIA-ChatBot/src/system/rag_orchestrator.py\", line 223, in reload_llm\n",
            "    self.chat_engine = ChatEngine(model_key)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ALIA-ChatBot/src/llm/chat_engine.py\", line 46, in __init__\n",
            "    self._load_model()\n",
            "  File \"/content/ALIA-ChatBot/src/llm/chat_engine.py\", line 63, in _load_model\n",
            "    self._load_gguf_model()\n",
            "  File \"/content/ALIA-ChatBot/src/llm/chat_engine.py\", line 373, in _load_gguf_model\n",
            "    self.model = Llama(\n",
            "                 ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py\", line 374, in __init__\n",
            "    internals.LlamaModel(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/llama_cpp/_internals.py\", line 58, in __init__\n",
            "    raise ValueError(f\"Failed to load model from file: {path_model}\")\n",
            "ValueError: Failed to load model from file: models/models--mradermacher--ALIA-40b-GGUF/snapshots/5a4ef045b0a73ec820e86cea0f411d2378de285c/ALIA-40b.Q8_0.gguf\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ALIA-ChatBot\n",
        "!python run.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}